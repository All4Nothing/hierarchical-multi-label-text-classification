# Multi-GPU Pipeline ì˜¤ë¥˜ ìˆ˜ì • ì‚¬í•­

## ğŸ“‹ ë°œê²¬ëœ ë¬¸ì œ ë° í•´ê²° ë°©ë²•

### 1. âš ï¸ FutureWarning: GradScaler API ë³€ê²½ (ê²½ê³ )

**ì›ì¸:**
```python
FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. 
Please use `torch.amp.GradScaler('cuda', args...)` instead.
```

PyTorch 2.0+ì—ì„œ Mixed Precision APIê°€ ë³€ê²½ë¨.

**í•´ê²° ë°©ë²•:**
```python
# ìˆ˜ì • ì „
from torch.cuda.amp import autocast, GradScaler
self.scaler = GradScaler()

# ìˆ˜ì • í›„
try:
    from torch.amp import autocast, GradScaler
except ImportError:
    from torch.cuda.amp import autocast, GradScaler

# ì´ˆê¸°í™” ì‹œ
if self.use_mixed_precision:
    try:
        self.scaler = GradScaler('cuda')  # PyTorch 2.0+
    except TypeError:
        self.scaler = GradScaler()  # PyTorch < 2.0
```

---

### 2. âš ï¸ FutureWarning: autocast API ë³€ê²½ (ê²½ê³ )

**ì›ì¸:**
```python
FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. 
Please use `torch.amp.autocast('cuda', args...)` instead.
```

**í•´ê²° ë°©ë²•:**
```python
# ìˆ˜ì • ì „
with autocast():
    outputs = model(inputs)

# ìˆ˜ì • í›„
try:
    with autocast('cuda'):  # PyTorch 2.0+
        outputs = model(inputs)
except TypeError:
    with autocast():  # PyTorch < 2.0
        outputs = model(inputs)
```

**ì ìš© ìœ„ì¹˜:**
- Training loop (Line ~633)
- Validation loop (Line ~700)

---

### 3. âš ï¸ Tokenizers Parallelism ê²½ê³  (ê²½ê³ )

**ì›ì¸:**
```
huggingface/tokenizers: The current process just got forked, 
after parallelism has already been used. Disabling parallelism to avoid deadlocks...
```

DataLoaderì˜ multi-workerì™€ HuggingFace tokenizerì˜ ë³‘ë ¬ ì²˜ë¦¬ê°€ ì¶©ëŒ.

**í•´ê²° ë°©ë²•:**
```python
# íŒŒì¼ ì‹œì‘ ë¶€ë¶„ì— í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€
import os
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

ì´ë ‡ê²Œ í•˜ë©´ tokenizerì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ë¹„í™œì„±í™”í•˜ê³ , DataLoaderì˜ multi-workerë§Œ ì‚¬ìš©.

---

### 4. ğŸ”´ Critical: Model Path ì˜¤ë¥˜ (ì¹˜ëª…ì )

**ì›ì¸:**
```python
HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 
'outputs/models/best_model'
```

Validation setì´ ì—†ì–´ì„œ `best_model`ì´ ì €ì¥ë˜ì§€ ì•ŠìŒ. `from_pretrained()`ê°€ ë¡œì»¬ ê²½ë¡œë¥¼ HuggingFace Hub repoë¡œ ì˜ëª» ì¸ì‹.

**í•´ê²° ë°©ë²• 1: best_model ìë™ ìƒì„±**
```python
# Training loopì—ì„œ
if self.val_loader:
    # Validation ê¸°ë°˜ìœ¼ë¡œ ì €ì¥
    if val_loss < best_loss:
        best_loss = val_loss
        save_path = os.path.join(output_dir, "best_model")
        self._save_model(save_path)
else:
    # Validationì´ ì—†ìœ¼ë©´ training loss ê¸°ë°˜ìœ¼ë¡œ ì €ì¥
    if avg_train_loss < best_loss:
        best_loss = avg_train_loss
        save_path = os.path.join(output_dir, "best_model")
        self._save_model(save_path)

# í•™ìŠµ ì™„ë£Œ í›„ fallback
best_model_path = os.path.join(output_dir, "best_model")
if not os.path.exists(best_model_path):
    logger.warning(f"best_model not found, copying final_model to best_model")
    self._save_model(best_model_path)
```

**í•´ê²° ë°©ë²• 2: local_files_only í”Œë˜ê·¸ ì‚¬ìš©**
```python
# Inference ì‹œ ëª¨ë¸ ë¡œë“œ
self.model = BertForSequenceClassification.from_pretrained(
    model_path,
    local_files_only=True  # ë¡œì»¬ ê²½ë¡œ ê°•ì œ
)

self.tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path,
    local_files_only=True
)
```

**í•´ê²° ë°©ë²• 3: ê²½ë¡œ ê²€ì¦ ì¶”ê°€**
```python
# ëª¨ë¸ ë¡œë“œ ì „ ê²½ë¡œ í™•ì¸
if not os.path.exists(model_path):
    raise FileNotFoundError(
        f"Model path '{model_path}' does not exist. "
        f"Make sure the model was saved during training."
    )
```

---

## âœ… ìˆ˜ì • ì™„ë£Œ ì‚¬í•­ ìš”ì•½

| ë¬¸ì œ | ì‹¬ê°ë„ | ìƒíƒœ | í•´ê²° ë°©ë²• |
|------|--------|------|----------|
| GradScaler API | ê²½ê³  | âœ… í•´ê²° | PyTorch ë²„ì „ë³„ ë¶„ê¸° ì²˜ë¦¬ |
| autocast API | ê²½ê³  | âœ… í•´ê²° | PyTorch ë²„ì „ë³„ ë¶„ê¸° ì²˜ë¦¬ |
| Tokenizers Parallelism | ê²½ê³  | âœ… í•´ê²° | í™˜ê²½ ë³€ìˆ˜ ì„¤ì • |
| best_model ë¯¸ìƒì„± | ì¹˜ëª…ì  | âœ… í•´ê²° | Training loss ê¸°ë°˜ ì €ì¥ + fallback |
| Model path ì¸ì‹ ì˜¤ë¥˜ | ì¹˜ëª…ì  | âœ… í•´ê²° | local_files_only + ê²½ë¡œ ê²€ì¦ |

---

## ğŸ” ì½”ë“œ ë³€ê²½ ì‚¬í•­ ìƒì„¸

### ë³€ê²½ 1: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```python
# Line ~18-20
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['USE_TF'] = 'NO'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # ì¶”ê°€
```

### ë³€ê²½ 2: Import ë¬¸ ìˆ˜ì •
```python
# Line ~30-35
try:
    from torch.amp import autocast, GradScaler
except ImportError:
    from torch.cuda.amp import autocast, GradScaler
```

### ë³€ê²½ 3: GradScaler ì´ˆê¸°í™”
```python
# Line ~524-533
if self.use_mixed_precision:
    try:
        self.scaler = GradScaler('cuda')
    except TypeError:
        self.scaler = GradScaler()
else:
    self.scaler = None
```

### ë³€ê²½ 4: autocast ì‚¬ìš© (Training)
```python
# Line ~633-645
if self.use_mixed_precision:
    try:
        with autocast('cuda'):
            outputs = self.model(...)
            loss = criterion(...)
    except TypeError:
        with autocast():
            outputs = self.model(...)
            loss = criterion(...)
```

### ë³€ê²½ 5: autocast ì‚¬ìš© (Validation)
```python
# Line ~700-711
if self.use_mixed_precision:
    try:
        with autocast('cuda'):
            outputs = self.model(...)
            loss = criterion(...)
    except TypeError:
        with autocast():
            outputs = self.model(...)
            loss = criterion(...)
```

### ë³€ê²½ 6: best_model ì €ì¥ ë¡œì§
```python
# Line ~668-690
if self.val_loader:
    # Validation ìˆì„ ë•Œ
    if val_loss < best_loss:
        save_path = os.path.join(output_dir, "best_model")
        self._save_model(save_path)
else:
    # Validation ì—†ì„ ë•Œ
    if avg_train_loss < best_loss:
        save_path = os.path.join(output_dir, "best_model")
        self._save_model(save_path)

# Fallback
best_model_path = os.path.join(output_dir, "best_model")
if not os.path.exists(best_model_path):
    self._save_model(best_model_path)
```

### ë³€ê²½ 7: ëª¨ë¸ ë¡œë“œ ì‹œ ê²½ë¡œ ê²€ì¦
```python
# Line ~761-773
if not os.path.exists(model_path):
    raise FileNotFoundError(...)

self.model = BertForSequenceClassification.from_pretrained(
    model_path,
    local_files_only=True
)

self.tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path,
    local_files_only=True
)
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼

### ìˆ˜ì • ì „
```
âš ï¸  FutureWarning: GradScaler deprecated (ë§¤ epochë§ˆë‹¤)
âš ï¸  FutureWarning: autocast deprecated (ë§¤ batchë§ˆë‹¤)
âš ï¸  Tokenizers parallelism warning (ë§¤ epochë§ˆë‹¤ 4ë²ˆ)
ğŸ”´ HFValidationError: Model path not found (CRASH!)
```

### ìˆ˜ì • í›„
```
âœ… ê²½ê³  ì—†ìŒ
âœ… ì •ìƒ í•™ìŠµ ì™„ë£Œ
âœ… best_model ìë™ ìƒì„±
âœ… ì¶”ë¡  ì •ìƒ ì‘ë™
```

---

## ğŸ“Š ì„±ëŠ¥ ì˜í–¥

ìˆ˜ì •ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ë³€í™”:
- **í•™ìŠµ ì†ë„**: ë³€í™” ì—†ìŒ (APIë§Œ ë³€ê²½)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ë³€í™” ì—†ìŒ
- **ì •í™•ë„**: ë³€í™” ì—†ìŒ
- **ì•ˆì •ì„±**: âœ… í–¥ìƒ (crash ë°©ì§€)

---

## ğŸ”„ í˜¸í™˜ì„±

| PyTorch ë²„ì „ | ìˆ˜ì • ì „ | ìˆ˜ì • í›„ |
|-------------|---------|---------|
| < 2.0 | âš ï¸ ê²½ê³  | âœ… ì •ìƒ |
| 2.0+ | âš ï¸ ê²½ê³  | âœ… ì •ìƒ |
| 2.1+ | ğŸ”´ ì˜¤ë¥˜ ê°€ëŠ¥ | âœ… ì •ìƒ |

---

## ğŸ’¡ ì¶”ê°€ ê¶Œì¥ ì‚¬í•­

### 1. Validation Set ì¶”ê°€
í˜„ì¬ëŠ” validationì´ ì—†ì–´ì„œ training lossë¡œ best modelì„ ì„ íƒí•©ë‹ˆë‹¤. ë” ë‚˜ì€ ë°©ë²•:

```python
# Train/Val split ì¶”ê°€
from sklearn.model_selection import train_test_split

train_texts_split, val_texts_split, train_labels_split, val_labels_split = \
    train_test_split(train_texts, train_labels, test_size=0.1, random_state=42)

trainer.prepare_data(
    train_texts_split, train_labels_split,
    val_texts_split, val_labels_split,  # Validation ì œê³µ
    batch_size=16
)
```

### 2. Early Stopping ì¶”ê°€
Overfitting ë°©ì§€:

```python
patience = 3
patience_counter = 0
best_val_loss = float('inf')

for epoch in range(num_epochs):
    # Training...
    
    if val_loader:
        val_loss = validate(...)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            save_model(...)
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("Early stopping!")
                break
```

### 3. ë¡œê¹… ê°œì„ 
Tensorboard ë˜ëŠ” Weights & Biases ì¶”ê°€:

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment_1')
writer.add_scalar('Loss/train', train_loss, epoch)
writer.add_scalar('Loss/val', val_loss, epoch)
```

---

## ğŸ“ ë³€ê²½ íŒŒì¼

- âœ… `pipeline_teleclass_multigpu.py` (7ê³³ ìˆ˜ì •)

---

## ğŸ¯ ê²°ë¡ 

ëª¨ë“  ê²½ê³ ì™€ ì˜¤ë¥˜ê°€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤:
- âœ… PyTorch 2.0+ í˜¸í™˜ì„± í™•ë³´
- âœ… Tokenizer ê²½ê³  ì œê±°
- âœ… Model ì €ì¥/ë¡œë“œ ì•ˆì •ì„± í–¥ìƒ
- âœ… Crash ë°©ì§€

**ìˆ˜ì •ëœ íŒŒì¼ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ê²½ê³  ì—†ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!** ğŸš€
