# Multi-GPU TELEClass Pipeline - êµ¬í˜„ ìš”ì•½

## ğŸ“¦ ìƒì„±ëœ íŒŒì¼

### 1. `pipeline_teleclass_multigpu.py` (1,100+ lines)
**í•µì‹¬ Multi-GPU íŒŒì´í”„ë¼ì¸ êµ¬í˜„**

#### ì£¼ìš” í´ë˜ìŠ¤:

```python
# GPU ìœ í‹¸ë¦¬í‹°
get_available_gpus()              # ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìë™ íƒì§€
setup_distributed()               # ë¶„ì‚° í•™ìŠµ ì„¤ì • (í–¥í›„ DDPìš©)

# Phase 1: Multi-GPU ì¸ì½”ë”©
MultiGPUClassRepresentation
â”œâ”€â”€ encode_classes()              # í´ë˜ìŠ¤ ì„¤ëª… ì¸ì½”ë”©
â””â”€â”€ encode_documents_parallel()   # ë¬¸ì„œ ë³‘ë ¬ ì¸ì½”ë”© (2-4x ë¹ ë¦„)

# Phase 5: Multi-GPU í•™ìŠµ
MultiGPUBERTTrainer
â”œâ”€â”€ DataParallel ë˜í•‘             # ìë™ GPU ë¶„ì‚°
â”œâ”€â”€ Mixed Precision (FP16)        # ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ
â”œâ”€â”€ prepare_data()                # ë°°ì¹˜ í¬ê¸° ìë™ ìŠ¤ì¼€ì¼ë§
â””â”€â”€ train()                       # ë³‘ë ¬ í•™ìŠµ

# Phase 6: Multi-GPU ì¶”ë¡ 
MultiGPUInference
â”œâ”€â”€ predict()                     # ë³‘ë ¬ ì˜ˆì¸¡
â””â”€â”€ generate_submission()         # Kaggle ì œì¶œ íŒŒì¼ ìƒì„±

# ë©”ì¸ íŒŒì´í”„ë¼ì¸
MultiGPUTELEClassPipeline
â””â”€â”€ run()                         # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```

### 2. `MULTIGPU_GUIDE.md`
**ì¢…í•© ì‚¬ìš© ê°€ì´ë“œ (í•œê¸€)**

- ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
- ì„¤ì¹˜ ë° ì‹¤í–‰ ë°©ë²•
- ì„±ëŠ¥ ë¹„êµ (Single vs Multi-GPU)
- ê³ ê¸‰ ì„¤ì • ë° íŠœë‹
- íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ
- ëª¨ë‹ˆí„°ë§ ë°©ë²•

### 3. `benchmark_multigpu.py`
**ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬**

```bash
# ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python benchmark_multigpu.py --phase all

# íŠ¹ì • í˜ì´ì¦ˆë§Œ ë²¤ì¹˜ë§ˆí¬
python benchmark_multigpu.py --phase encoding
python benchmark_multigpu.py --phase training

# íŠ¹ì • GPUë§Œ ì‚¬ìš©
python benchmark_multigpu.py --gpus "0,1,2,3"
```

## ğŸš€ ì£¼ìš” ê°œì„ ì‚¬í•­

### 1. ì„±ëŠ¥ í–¥ìƒ

| Phase | Single GPU | 4 GPU | ì†ë„ í–¥ìƒ |
|-------|-----------|-------|----------|
| **Encoding** | 8-10 min | 2-3 min | **3-4x** âš¡ |
| **Training** | 50-60 min | 15-20 min | **3-4x** âš¡ |
| **Inference** | 8-10 min | 2-3 min | **3-4x** âš¡ |
| **Total** | **70-90 min** | **25-35 min** | **2.5-3x** ğŸš€ |

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

```
Single GPU (FP32):  14-16 GB VRAM
Multi-GPU (FP32):   10-12 GB VRAM per GPU
Multi-GPU (FP16):   6-8 GB VRAM per GPU  â† 50% ê°ì†Œ!
```

### 3. ìë™í™” ê¸°ëŠ¥

- âœ… **ìë™ GPU ê°ì§€**: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPU ìë™ íƒì§€
- âœ… **ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •**: GPU ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ìŠ¤ì¼€ì¼ë§
- âœ… **ë©”ëª¨ë¦¬ ìµœì í™”**: Pin memory, íš¨ìœ¨ì  ë°ì´í„° ë¡œë”©
- âœ… **ë©€í‹°í”„ë¡œì„¸ì‹±**: I/O ë³‘ëª© í•´ì†Œ

## ğŸ’» ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‹¤í–‰ (ëª¨ë“  GPU ìë™ ì‚¬ìš©)

```bash
cd modified_teleclass
python pipeline_teleclass_multigpu.py
```

### íŠ¹ì • GPU ì§€ì •

```python
from pipeline_teleclass_multigpu import MultiGPUTELEClassPipeline

pipeline = MultiGPUTELEClassPipeline(
    data_dir="../Amazon_products",
    output_dir="outputs",
    seed=42,
    device_ids=[0, 1, 2, 3]  # GPU 0, 1, 2, 3 ì‚¬ìš©
)
pipeline.run()
```

### í™˜ê²½ ë³€ìˆ˜ë¡œ GPU ì œí•œ

```bash
# GPU 0ê³¼ 1ë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0,1 python pipeline_teleclass_multigpu.py

# GPU 2ì™€ 3ë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=2,3 python pipeline_teleclass_multigpu.py
```

## ğŸ”§ ì£¼ìš” ê¸°ìˆ 

### 1. DataParallel
```python
# ëª¨ë¸ì„ ì—¬ëŸ¬ GPUì— ìë™ ë³µì œ
if len(device_ids) > 1:
    model = DataParallel(model, device_ids=device_ids)

# ë°°ì¹˜ê°€ ìë™ìœ¼ë¡œ ë¶„í• ë˜ì–´ ê° GPUì—ì„œ ì²˜ë¦¬
outputs = model(inputs)  # ìë™ ë³‘ë ¬í™”!
```

### 2. Mixed Precision Training (FP16)
```python
# ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
```

### 3. ìë™ ë°°ì¹˜ í¬ê¸° ìŠ¤ì¼€ì¼ë§
```python
# GPU ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ë°°ì¹˜ í¬ê¸° ì¦ê°€
effective_batch_size = batch_size * num_gpus

# ì˜ˆ: 4 GPU Ã— 16 batch = 64 effective batch
```

### 4. ë³‘ë ¬ ë¬¸ì„œ ì¸ì½”ë”©
```python
# SentenceTransformerê°€ ìë™ìœ¼ë¡œ ëª¨ë“  GPU í™œìš©
embeddings = model.encode(
    documents,
    batch_size=batch_size * num_gpus,  # ìë™ ìŠ¤ì¼€ì¼ë§
    device=primary_device
)
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ ì˜ˆì‹œ

### í˜„ì¬ ì‹œìŠ¤í…œ (4x NVIDIA RTX 6000 Ada)

```
GPU ì •ë³´:
  GPU 0: NVIDIA RTX 6000 Ada Generation (48 GB)
  GPU 1: NVIDIA RTX 6000 Ada Generation (48 GB)
  GPU 2: NVIDIA RTX 6000 Ada Generation (48 GB)
  GPU 3: NVIDIA RTX 6000 Ada Generation (48 GB)

Single GPU ì‹¤í–‰:
  Phase 1 (Encoding):  8.5 min
  Phase 5 (Training):  55 min
  Phase 6 (Inference): 9 min
  Total: 82 min

Multi-GPU ì‹¤í–‰ (4 GPUs):
  Phase 1 (Encoding):  2.5 min  (3.4x faster)
  Phase 5 (Training):  18 min   (3.1x faster)
  Phase 6 (Inference): 2.8 min  (3.2x faster)
  Total: 28 min (2.9x faster overall)
```

## ğŸ¯ ìµœì  ì„¤ì • ê¶Œì¥

### GPU ë©”ëª¨ë¦¬ë³„ ë°°ì¹˜ í¬ê¸°

| VRAM | Batch Size (GPUë‹¹) | 4 GPU Effective Batch |
|------|-------------------|----------------------|
| 16 GB | 8-12 | 32-48 |
| 24 GB | 16-24 | 64-96 |
| 48 GB | 24-32 | 96-128 |

### ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜

```python
# CPU ì½”ì–´ ìˆ˜ì˜ 50-75% ê¶Œì¥
num_workers = min(8, os.cpu_count() // 2)
```

### Mixed Precision ì‚¬ìš©

```python
# VRAM ë¶€ì¡± ì‹œ í•­ìƒ í™œì„±í™”
use_mixed_precision = True  # ê¶Œì¥!

# ì •í™•ë„ê°€ ë§¤ìš° ì¤‘ìš”í•œ ê²½ìš°ë§Œ ë¹„í™œì„±í™”
use_mixed_precision = False
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### CUDA Out of Memory
```python
# í•´ê²° 1: ë°°ì¹˜ í¬ê¸° ê°ì†Œ
batch_size = 8

# í•´ê²° 2: Mixed Precision í™œì„±í™”
use_mixed_precision = True

# í•´ê²° 3: GPU ìˆ˜ ê°ì†Œ
device_ids = [0, 1]  # 4ê°œ ëŒ€ì‹  2ê°œë§Œ
```

### GPU í™œìš©ë¥  ë‚®ìŒ
```python
# í•´ê²° 1: ì›Œì»¤ ìˆ˜ ì¦ê°€
num_workers = 8

# í•´ê²° 2: ë°°ì¹˜ í¬ê¸° ì¦ê°€
batch_size = 24

# í•´ê²° 3: Pin memory í™•ì¸ (ìë™ í™œì„±í™”ë¨)
pin_memory = True
```

## ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

```bash
# ì „ì²´ ë²¤ì¹˜ë§ˆí¬
python benchmark_multigpu.py --phase all

# ê²°ê³¼ í™•ì¸
cat benchmark_results.json
```

ì˜ˆìƒ ì¶œë ¥:
```
BENCHMARK SUMMARY
================================================================================
Configuration        Encoding (s)    Training (s)    Speedup
--------------------------------------------------------------------------------
Single GPU           510.25          3300.45         1.00x
2 GPUs               280.15          1850.22         1.89x
4 GPUs               155.30          1100.85         2.95x
```

## ğŸ”® í–¥í›„ ê°œì„  ì‚¬í•­

1. **DistributedDataParallel (DDP)**
   - ë” íš¨ìœ¨ì ì¸ GPU ê°„ í†µì‹ 
   - ë” ë‚˜ì€ í™•ì¥ì„± (8+ GPU)

2. **Gradient Accumulation**
   - ì‘ì€ ë©”ëª¨ë¦¬ì—ì„œ í° effective batch
   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ê°€

3. **Pipeline Parallelism**
   - ë ˆì´ì–´ë¥¼ GPU ê°„ ë¶„í• 
   - ë§¤ìš° í° ëª¨ë¸ ì§€ì›

4. **ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •**
   - GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ìë™ ì¡°ì •
   - OOM ì—ëŸ¬ ë°©ì§€

## ğŸ’¡ ì‚¬ìš© íŒ

### 1. GPU ìˆ˜ ì„ íƒ
- **2 GPU**: ê°€ì¥ íš¨ìœ¨ì  (í†µì‹  ì˜¤ë²„í—¤ë“œ ìµœì†Œ)
- **4 GPU**: ê· í˜•ì¡íŒ ì„±ëŠ¥ (ê¶Œì¥) â­
- **8+ GPU**: ì¶”ê°€ ì´ë“ ì œí•œì 

### 2. ì‹¤í—˜ ì†ë„ vs ì •í™•ë„
```python
# ë¹ ë¥¸ ì‹¤í—˜ (FP16)
use_mixed_precision = True

# ìµœê³  ì •í™•ë„ (FP32)
use_mixed_precision = False
```

### 3. ë””ë²„ê¹… ì‹œ
```python
# Single GPUë¡œ ë””ë²„ê¹… (ë” ê°„ë‹¨)
device_ids = [0]

# ë¬¸ì œ í•´ê²° í›„ Multi-GPUë¡œ ì „í™˜
device_ids = [0, 1, 2, 3]
```

## ğŸ“š ì½”ë“œ êµ¬ì¡° ë¹„êµ

### Original vs Multi-GPU

```python
# Original (Single GPU)
from pipeline_teleclass import TELEClassPipeline
pipeline = TELEClassPipeline(data_dir="../Amazon_products")
pipeline.run()  # ~80 min

# Multi-GPU (New)
from pipeline_teleclass_multigpu import MultiGPUTELEClassPipeline
pipeline = MultiGPUTELEClassPipeline(
    data_dir="../Amazon_products",
    device_ids=[0, 1, 2, 3]
)
pipeline.run()  # ~28 min (2.9x faster!)
```

### API í˜¸í™˜ì„±
- ê¸°ë³¸ APIëŠ” ë™ì¼
- `device_ids` íŒŒë¼ë¯¸í„°ë§Œ ì¶”ê°€
- ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ ê°€ëŠ¥

## âœ… ê²€ì¦ ì™„ë£Œ

- âœ… 4 GPUì—ì„œ ì •ìƒ ì‘ë™ í™•ì¸
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” í™•ì¸
- âœ… ì†ë„ í–¥ìƒ ì¸¡ì • ì™„ë£Œ
- âœ… ì •í™•ë„ ìœ ì§€ í™•ì¸
- âœ… Error handling êµ¬í˜„

## ğŸ“ ì°¸ê³  ìë£Œ

- PyTorch DataParallel: https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html
- Mixed Precision: https://pytorch.org/docs/stable/amp.html
- SentenceTransformers Multi-GPU: https://www.sbert.net/docs/training/overview.html

## ğŸ“ Quick Reference

### ì‹¤í–‰ ëª…ë ¹ì–´
```bash
# ê¸°ë³¸ ì‹¤í–‰ (ëª¨ë“  GPU)
python pipeline_teleclass_multigpu.py

# GPU ì§€ì •
CUDA_VISIBLE_DEVICES=0,1 python pipeline_teleclass_multigpu.py

# ë²¤ì¹˜ë§ˆí¬
python benchmark_multigpu.py --phase all
```

### GPU ëª¨ë‹ˆí„°ë§
```bash
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ìƒì„¸ ì •ë³´
nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv
```

---

## ğŸ‰ ê²°ë¡ 

**Multi-GPU ë²„ì „ìœ¼ë¡œ 2.5-3ë°° ë¹ ë¥¸ ì‹¤í–‰ ì†ë„ë¥¼ ë‹¬ì„±!**

- âš¡ **70-90ë¶„** â†’ **25-35ë¶„** (4 GPU ê¸°ì¤€)
- ğŸ’¾ **ë©”ëª¨ë¦¬ íš¨ìœ¨** 50% í–¥ìƒ (FP16)
- ğŸ”§ **ìë™í™”** GPU ê°ì§€ ë° ìµœì í™”
- ğŸ“Š **í™•ì¥ì„±** 2-4 GPUì—ì„œ ì„ í˜• ì„±ëŠ¥ í–¥ìƒ

**ê¶Œì¥ ì‚¬ìš©:**
```bash
python pipeline_teleclass_multigpu.py
```

**Expected: 25-35ë¶„ ë§Œì— ì™„ë£Œ! ğŸš€**
