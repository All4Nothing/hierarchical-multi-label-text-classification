# Multi-GPU TELEClass Pipeline Guide

## Overview

`pipeline_teleclass_multigpu.py`ëŠ” ì—¬ëŸ¬ GPUë¥¼ í™œìš©í•˜ì—¬ TELEClass íŒŒì´í”„ë¼ì¸ì„ **2-4ë°° ë¹ ë¥´ê²Œ** ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìµœì í™”ëœ ë²„ì „ì…ë‹ˆë‹¤.

## ì£¼ìš” ê°œì„ ì‚¬í•­

### ğŸš€ ì„±ëŠ¥ í–¥ìƒ

1. **Multi-GPU ë¬¸ì„œ ì¸ì½”ë”©** (Phase 1)
   - SentenceTransformerê°€ ëª¨ë“  ê°€ìš© GPUì— ìë™ìœ¼ë¡œ ì›Œí¬ë¡œë“œ ë¶„ì‚°
   - ë°°ì¹˜ í¬ê¸°ê°€ GPU ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ìë™ ì¦ê°€
   - ì˜ˆ: 4 GPU Ã— 64 batch = 256 effective batch size

2. **DataParallel BERT í•™ìŠµ** (Phase 5)
   - ì—¬ëŸ¬ GPUì— ê±¸ì³ ë°°ì¹˜ë¥¼ ìë™ìœ¼ë¡œ ë¶„í• 
   - ê° GPUê°€ ë…ë¦½ì ìœ¼ë¡œ forward/backward pass ìˆ˜í–‰
   - ê·¸ë˜ë””ì–¸íŠ¸ ìë™ ì§‘ê³„ ë° ë™ê¸°í™”

3. **Mixed Precision Training (FP16)**
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~50% ê°ì†Œ
   - í•™ìŠµ ì†ë„ ~30-50% í–¥ìƒ
   - NVIDIA Tensor Core í™œìš©

4. **Multi-GPU ì¶”ë¡ ** (Phase 6)
   - í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ GPUì— ë¶„ì‚°
   - ë°°ì¹˜ í¬ê¸° ìë™ ìŠ¤ì¼€ì¼ë§
   - ë³‘ë ¬ ì˜ˆì¸¡ìœ¼ë¡œ ì¶”ë¡  ì‹œê°„ ë‹¨ì¶•

### ğŸ¯ ìë™ ìµœì í™”

- **ìë™ GPU ê°ì§€**: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPU ìë™ íƒì§€
- **ë™ì  ë°°ì¹˜ í¬ê¸°**: GPU ìˆ˜ì— ë”°ë¼ ìë™ ì¡°ì •
- **ë©”ëª¨ë¦¬ ìµœì í™”**: Pin memory ë° íš¨ìœ¨ì ì¸ ë°ì´í„° ë¡œë”©
- **ì›Œì»¤ í”„ë¡œì„¸ìŠ¤**: ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ I/O ë³‘ëª© í•´ì†Œ

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- **GPU**: NVIDIA GPU 2ê°œ ì´ìƒ ê¶Œì¥ (1ê°œë„ ê°€ëŠ¥)
- **VRAM**: GPUë‹¹ ìµœì†Œ 16GB ê¶Œì¥
- **RAM**: 32GB+ ê¶Œì¥
- **CUDA**: 11.0 ì´ìƒ

### ì†Œí”„íŠ¸ì›¨ì–´
```bash
torch>=2.0.0
transformers>=4.30.0
sentence-transformers>=2.2.0
pandas>=1.5.0
numpy>=1.23.0
networkx>=3.0
scikit-learn>=1.2.0
tqdm>=4.65.0
```

## ì„¤ì¹˜ ë° ì‹¤í–‰

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
cd modified_teleclass
pip install -r requirements_teleclass.txt
```

### 2. GPU í™•ì¸

```python
import torch
print(f"Available GPUs: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
```

ì˜ˆìƒ ì¶œë ¥:
```
Available GPUs: 4
  GPU 0: NVIDIA RTX 6000 Ada Generation
  GPU 1: NVIDIA RTX 6000 Ada Generation
  GPU 2: NVIDIA RTX 6000 Ada Generation
  GPU 3: NVIDIA RTX 6000 Ada Generation
```

### 3. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

#### ë°©ë²• 1: ëª¨ë“  GPU ìë™ ì‚¬ìš©
```bash
python pipeline_teleclass_multigpu.py
```

#### ë°©ë²• 2: íŠ¹ì • GPU ì§€ì •
```python
from pipeline_teleclass_multigpu import MultiGPUTELEClassPipeline

# GPU 0, 1, 2ë§Œ ì‚¬ìš©
pipeline = MultiGPUTELEClassPipeline(
    data_dir="../Amazon_products",
    output_dir="outputs",
    seed=42,
    device_ids=[0, 1, 2]  # ì›í•˜ëŠ” GPU ID ì§€ì •
)
pipeline.run()
```

#### ë°©ë²• 3: í™˜ê²½ ë³€ìˆ˜ë¡œ GPU ì œí•œ
```bash
# GPU 0ê³¼ 1ë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0,1 python pipeline_teleclass_multigpu.py

# GPU 2ì™€ 3ë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=2,3 python pipeline_teleclass_multigpu.py
```

## ì„±ëŠ¥ ë¹„êµ

### ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (4x NVIDIA RTX 6000 Ada)

| Phase | Single GPU | 4 GPU Multi-GPU | ì†ë„ í–¥ìƒ |
|-------|-----------|-----------------|----------|
| Phase 1: Encoding | 8-10 min | 2-3 min | **~3-4x** |
| Phase 2: Refinement | 3-5 min | 3-5 min | ~1x (GPU ê°„ í†µì‹  ì˜¤ë²„í—¤ë“œ) |
| Phase 3: Augmentation | 1 min | 1 min | ~1x |
| Phase 4: Hierarchy | 1 min | 1 min | ~1x |
| Phase 5: BERT Training | 50-60 min | 15-20 min | **~3-4x** |
| Phase 6: Inference | 8-10 min | 2-3 min | **~3-4x** |
| **Total** | **70-90 min** | **25-35 min** | **~2.5-3x** |

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| ì„¤ì • | GPUë‹¹ VRAM ì‚¬ìš©ëŸ‰ |
|------|------------------|
| Single GPU (FP32) | ~14-16 GB |
| Multi-GPU (FP32) | ~10-12 GB |
| Multi-GPU (FP16) | ~6-8 GB |

## ì£¼ìš” ê¸°ëŠ¥ ì„¤ëª…

### 1. MultiGPUClassRepresentation

**ë¬¸ì„œ ì¸ì½”ë”© ë³‘ë ¬í™”:**
```python
class_repr = MultiGPUClassRepresentation(device_ids=[0, 1, 2, 3])

# ìë™ìœ¼ë¡œ 4ê°œ GPUì— ë¶„ì‚°
doc_embeddings = class_repr.encode_documents_parallel(
    all_corpus,
    batch_size=64  # GPUë‹¹ 64, ì´ 256 effective batch
)
```

**ì‘ë™ ë°©ì‹:**
- SentenceTransformerê°€ ë‚´ë¶€ì ìœ¼ë¡œ DataParallel ì‚¬ìš©
- ê° GPUê°€ ë°°ì¹˜ì˜ ì¼ë¶€ë¥¼ ì²˜ë¦¬
- ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ì§‘ê³„

### 2. MultiGPUBERTTrainer

**DataParallel í•™ìŠµ:**
```python
trainer = MultiGPUBERTTrainer(
    num_classes=531,
    device_ids=[0, 1, 2, 3],
    use_mixed_precision=True  # FP16 í™œì„±í™”
)

trainer.prepare_data(
    train_texts, 
    train_labels, 
    batch_size=16  # GPUë‹¹ 16, ì´ 64 effective batch
)

trainer.train(num_epochs=3)
```

**ì‘ë™ ë°©ì‹:**
1. ëª¨ë¸ì´ DataParallelë¡œ ë˜í•‘ë¨
2. ê° ë°°ì¹˜ê°€ GPU ìˆ˜ë§Œí¼ ë¶„í• 
3. ê° GPUê°€ forward pass ìˆ˜í–‰
4. Lossê°€ primary GPUì—ì„œ ì§‘ê³„
5. Backward pass í›„ ê·¸ë˜ë””ì–¸íŠ¸ ë™ê¸°í™”
6. Optimizer step ìˆ˜í–‰

**Mixed Precision Training:**
```python
# FP16 ì—°ì‚°ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ
with autocast():
    outputs = model(input_ids, attention_mask)
    loss = criterion(outputs.logits, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 3. MultiGPUInference

**ë³‘ë ¬ ì¶”ë¡ :**
```python
inference = MultiGPUInference(
    model_path="outputs/models/best_model",
    device_ids=[0, 1, 2, 3]
)

# ìë™ìœ¼ë¡œ 4ê°œ GPUì— ë¶„ì‚°
predictions = inference.predict(
    test_corpus,
    batch_size=32  # GPUë‹¹ 32, ì´ 128 effective batch
)
```

## ê³ ê¸‰ ì„¤ì •

### 1. ë°°ì¹˜ í¬ê¸° íŠœë‹

```python
# ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ê°ì†Œ
trainer.prepare_data(train_texts, train_labels, batch_size=8)

# ë©”ëª¨ë¦¬ ì—¬ìœ  ì‹œ ì¦ê°€
trainer.prepare_data(train_texts, train_labels, batch_size=32)
```

**ê¶Œì¥ ë°°ì¹˜ í¬ê¸°:**
- VRAM 16GB: batch_size=16
- VRAM 24GB: batch_size=24-32
- VRAM 48GB: batch_size=32-48

### 2. ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì¡°ì •

```python
# CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •
trainer.prepare_data(
    train_texts, 
    train_labels, 
    num_workers=8  # CPU ì½”ì–´ ìˆ˜ì˜ 50-75%
)
```

### 3. Mixed Precision ë¹„í™œì„±í™”

```python
# ì •í™•ë„ê°€ ì¤‘ìš”í•œ ê²½ìš°
trainer = MultiGPUBERTTrainer(
    num_classes=531,
    use_mixed_precision=False  # FP32 ì‚¬ìš©
)
```

### 4. íŠ¹ì • Phaseë§Œ Multi-GPU ì‚¬ìš©

```python
# Phase 1: Multi-GPU ì¸ì½”ë”©ë§Œ ì‚¬ìš©
class_repr = MultiGPUClassRepresentation(device_ids=[0, 1])
doc_embeddings = class_repr.encode_documents_parallel(all_corpus)

# Phase 5: Single GPU í•™ìŠµ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)
trainer = MultiGPUBERTTrainer(device_ids=[0])
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Issue 1: CUDA Out of Memory

**ì¦ìƒ:**
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```

**í•´ê²°ì±…:**
```python
# 1. ë°°ì¹˜ í¬ê¸° ê°ì†Œ
trainer.prepare_data(train_texts, train_labels, batch_size=8)

# 2. Mixed Precision í™œì„±í™”
use_mixed_precision=True

# 3. ì‚¬ìš© GPU ìˆ˜ ê°ì†Œ
device_ids=[0, 1]  # 4ê°œ ëŒ€ì‹  2ê°œë§Œ ì‚¬ìš©
```

### Issue 2: GPU ê°„ ì„±ëŠ¥ ë¶ˆê· í˜•

**ì¦ìƒ:**
```
GPU 0: 95% utilization
GPU 1: 30% utilization
GPU 2: 25% utilization
GPU 3: 20% utilization
```

**í•´ê²°ì±…:**
```python
# DataParallel ëŒ€ì‹  DistributedDataParallel ê³ ë ¤ (í–¥í›„ êµ¬í˜„)
# í˜„ì¬ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬ ì™„í™”
batch_size=16  # ë” í° ë°°ì¹˜ë¡œ ê· ë“± ë¶„ì‚°
```

### Issue 3: ëŠë¦° ë°ì´í„° ë¡œë”©

**ì¦ìƒ:**
```
GPU utilization: 30-40% (should be 80-100%)
```

**í•´ê²°ì±…:**
```python
# ì›Œì»¤ ìˆ˜ ì¦ê°€
num_workers=8  # ê¸°ë³¸ê°’ 4ì—ì„œ ì¦ê°€

# Pin memory í™œì„±í™” (ìë™ í™œì„±í™”ë¨)
pin_memory=True
```

### Issue 4: Multi-GPUì—ì„œ ì„±ëŠ¥ í–¥ìƒ ì—†ìŒ

**ì ê²€ ì‚¬í•­:**
1. GPU ê°„ í†µì‹  ëŒ€ì—­í­ í™•ì¸:
   ```bash
   nvidia-smi topo -m
   ```

2. PCIe ì—°ê²° ìƒíƒœ í™•ì¸:
   ```bash
   nvidia-smi
   # Link: P2P ë˜ëŠ” SYS í™•ì¸
   ```

3. ë°°ì¹˜ í¬ê¸°ê°€ ì¶©ë¶„íˆ í°ì§€ í™•ì¸:
   ```python
   # ë„ˆë¬´ ì‘ì€ ë°°ì¹˜ëŠ” ì˜¤ë²„í—¤ë“œ ì¦ê°€
   batch_size=16  # ìµœì†Œ ê¶Œì¥ê°’
   ```

## ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§

```bash
# í„°ë¯¸ë„ 1: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python pipeline_teleclass_multigpu.py

# í„°ë¯¸ë„ 2: GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

### Python ì½”ë“œë¡œ ëª¨ë‹ˆí„°ë§

```python
import subprocess
import time

def monitor_gpus():
    while True:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        print(result.stdout)
        time.sleep(1)

# ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. ìµœì ì˜ GPU ìˆ˜ ì„ íƒ

- **2 GPU**: ê°€ì¥ íš¨ìœ¨ì  (í†µì‹  ì˜¤ë²„í—¤ë“œ ìµœì†Œ)
- **4 GPU**: ê· í˜•ì¡íŒ ì„±ëŠ¥ (ê¶Œì¥)
- **8+ GPU**: ì¶”ê°€ ì´ë“ ì œí•œì  (í†µì‹  ì˜¤ë²„í—¤ë“œ ì¦ê°€)

### 2. ë°°ì¹˜ í¬ê¸° ìµœì í™”

```python
# GPU ìˆ˜ Ã— 16 ë˜ëŠ” 32ê°€ ì¼ë°˜ì ìœ¼ë¡œ ìµœì 
# 4 GPU: batch_size=16 â†’ effective=64
# 4 GPU: batch_size=32 â†’ effective=128
```

### 3. ë°ì´í„° ë¡œë”© ìµœì í™”

```python
num_workers = min(8, os.cpu_count() // 2)
pin_memory = True  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
```

### 4. Gradient Accumulation (ëŒ€ì²´ ë°©ë²•)

ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ì§€ë§Œ í° effective batchë¥¼ ì›í•  ë•Œ:

```python
# í–¥í›„ ì¶”ê°€ êµ¬í˜„ ì˜ˆì •
accumulation_steps = 4
effective_batch = batch_size * accumulation_steps * num_gpus
```

## ë¹„êµ: Single vs Multi-GPU

### ì–¸ì œ Multi-GPUë¥¼ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?

**Multi-GPU ì‚¬ìš© ê¶Œì¥:**
- âœ… GPUê°€ 2ê°œ ì´ìƒ ìˆì„ ë•Œ
- âœ… ë¹ ë¥¸ ì‹¤í—˜ ë°˜ë³µì´ í•„ìš”í•  ë•Œ
- âœ… í° ë°°ì¹˜ í¬ê¸°ê°€ í•„ìš”í•  ë•Œ
- âœ… ë¬¸ì„œ ìˆ˜ê°€ ë§ì„ ë•Œ (50K+)

**Single GPU ì‚¬ìš© ê¶Œì¥:**
- âœ… GPUê°€ 1ê°œë§Œ ìˆì„ ë•Œ
- âœ… ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•  ë•Œ
- âœ… ì‘ì€ ë°ì´í„°ì…‹ (<10K ë¬¸ì„œ)
- âœ… ë””ë²„ê¹… ì‹œ

### ì½”ë“œ ì°¨ì´

```python
# Single GPU
from pipeline_teleclass import TELEClassPipeline
pipeline = TELEClassPipeline(data_dir="../Amazon_products")
pipeline.run()

# Multi-GPU
from pipeline_teleclass_multigpu import MultiGPUTELEClassPipeline
pipeline = MultiGPUTELEClassPipeline(
    data_dir="../Amazon_products",
    device_ids=[0, 1, 2, 3]  # ë˜ëŠ” Noneìœ¼ë¡œ ìë™
)
pipeline.run()
```

## ì¶”ê°€ ê°œì„  ì‚¬í•­ (í–¥í›„)

í˜„ì¬ êµ¬í˜„ì—ì„œ ë” ê°œì„ í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„:

1. **DistributedDataParallel (DDP)**
   - DataParallelë³´ë‹¤ ë” íš¨ìœ¨ì 
   - GPU ê°„ í†µì‹  ìµœì í™”
   - ë” ë‚˜ì€ í™•ì¥ì„±

2. **Gradient Accumulation**
   - ì‘ì€ GPU ë©”ëª¨ë¦¬ì—ì„œ í° effective batch
   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ê°€

3. **Model Parallelism**
   - ë§¤ìš° í° ëª¨ë¸ì„ GPU ê°„ ë¶„í• 
   - ë” í° ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥

4. **Pipeline Parallelism**
   - ë ˆì´ì–´ë¥¼ GPU ê°„ ë¶„í• 
   - ì§€ì†ì ì¸ GPU í™œìš©

## ì°¸ê³  ìë£Œ

- [PyTorch DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)
- [PyTorch Mixed Precision](https://pytorch.org/docs/stable/amp.html)
- [SentenceTransformers Multi-GPU](https://www.sbert.net/docs/training/overview.html#multi-gpu-training)

## ìš”ì•½

| íŠ¹ì§• | Single GPU | Multi-GPU |
|------|-----------|-----------|
| ì‹¤í–‰ ì‹œê°„ | 70-90 ë¶„ | 25-35 ë¶„ |
| GPU ë©”ëª¨ë¦¬ | 14-16 GB | 6-8 GB (FP16) |
| ì„¤ì • ë³µì¡ë„ | â­ ê°„ë‹¨ | â­â­ ë³´í†µ |
| ë””ë²„ê¹… | â­â­â­ ì‰¬ì›€ | â­â­ ë³´í†µ |
| í™•ì¥ì„± | ì œí•œì  | ìš°ìˆ˜ |
| ê¶Œì¥ ì‚¬ìš© | í”„ë¡œí† íƒ€ì…, ë””ë²„ê¹… | í”„ë¡œë•ì…˜, ëŒ€ê·œëª¨ |

---

**Quick Start:**
```bash
cd modified_teleclass
python pipeline_teleclass_multigpu.py
```

**Expected:** ~25-35ë¶„ ì†Œìš”, 4 GPU ì‚¬ìš© ì‹œ ~3ë°° ì†ë„ í–¥ìƒ! ğŸš€
