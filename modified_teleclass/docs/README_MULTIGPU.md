# Multi-GPU TELEClass Pipeline

## ğŸ¯ ê°œìš”

ì—¬ëŸ¬ ê°œì˜ GPUë¥¼ í™œìš©í•˜ì—¬ TELEClass íŒŒì´í”„ë¼ì¸ì„ **2.5-3ë°° ë¹ ë¥´ê²Œ** ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ìµœì í™”ëœ ë²„ì „ì…ë‹ˆë‹¤.

### ì£¼ìš” ì„±ëŠ¥ ê°œì„ 

```
Single GPU: 70-90ë¶„  â†’  Multi-GPU (4ê°œ): 25-35ë¶„  (âš¡ 2.5-3x ë¹ ë¦„!)
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 16GB  â†’  6-8GB per GPU (FP16 ì‚¬ìš© ì‹œ)
```

---

## ğŸ“¦ ìƒì„±ëœ íŒŒì¼

| íŒŒì¼ | í¬ê¸° | ì„¤ëª… |
|------|------|------|
| `pipeline_teleclass_multigpu.py` | 38KB | í•µì‹¬ Multi-GPU íŒŒì´í”„ë¼ì¸ (1,100+ lines) |
| `MULTIGPU_GUIDE.md` | 11KB | ì¢…í•© ì‚¬ìš© ê°€ì´ë“œ (í•œê¸€) |
| `MULTIGPU_SUMMARY.md` | 9.1KB | êµ¬í˜„ ìš”ì•½ ë° ë¹„êµ |
| `benchmark_multigpu.py` | 12KB | ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ |
| `GPU_ARCHITECTURE.txt` | 21KB | ì•„í‚¤í…ì²˜ ì‹œê°í™” (ASCII) |

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ ì‹¤í–‰ (ëª¨ë“  GPU ìë™ ì‚¬ìš©)

```bash
cd modified_teleclass
python pipeline_teleclass_multigpu.py
```

### 2. íŠ¹ì • GPU ì§€ì •

```bash
# GPU 0, 1, 2, 3ë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0,1,2,3 python pipeline_teleclass_multigpu.py

# GPU 0, 1ë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0,1 python pipeline_teleclass_multigpu.py
```

### 3. Python ì½”ë“œë¡œ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
from pipeline_teleclass_multigpu import MultiGPUTELEClassPipeline

pipeline = MultiGPUTELEClassPipeline(
    data_dir="../Amazon_products",
    output_dir="outputs",
    seed=42,
    device_ids=[0, 1, 2, 3]  # ì‚¬ìš©í•  GPU ì§€ì •
)

pipeline.run()
```

---

## ğŸ’¡ ì£¼ìš” ê¸°ëŠ¥

### 1. ìë™ GPU ê°ì§€ ë° ë¶„ì‚°
```python
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  GPU ìë™ íƒì§€
available_gpus = get_available_gpus()
# ì¶œë ¥: [0, 1, 2, 3]
```

### 2. DataParallel í•™ìŠµ
- ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ëª¨ë“  GPUì— ë³µì œ
- ë°°ì¹˜ê°€ GPU ìˆ˜ë§Œí¼ ìë™ ë¶„í• 
- ê·¸ë˜ë””ì–¸íŠ¸ ìë™ ì§‘ê³„ ë° ë™ê¸°í™”

### 3. Mixed Precision (FP16)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ê°ì†Œ
- í•™ìŠµ ì†ë„ 30-50% í–¥ìƒ
- ìë™ìœ¼ë¡œ í™œì„±í™”ë¨

### 4. ìµœì í™”ëœ ë°ì´í„° ë¡œë”©
- ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤ (num_workers=4)
- Pin memory for GPU transfer
- ë°°ì¹˜ í¬ê¸° ìë™ ìŠ¤ì¼€ì¼ë§

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### ì‹¤í–‰ ì‹œê°„ (4x NVIDIA RTX 6000 Ada ê¸°ì¤€)

| Phase | Single GPU | 4 GPU | ì†ë„ í–¥ìƒ |
|-------|-----------|-------|----------|
| Phase 1: Encoding | 8-10 min | 2-3 min | **3-4x** âš¡ |
| Phase 2: Refinement | 3-5 min | 3-5 min | ~1x |
| Phase 5: Training | 50-60 min | 15-20 min | **3-4x** âš¡ |
| Phase 6: Inference | 8-10 min | 2-3 min | **3-4x** âš¡ |
| **Total** | **70-90 min** | **25-35 min** | **2.5-3x** ğŸš€ |

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

| ì„¤ì • | GPUë‹¹ VRAM |
|------|-----------|
| Single GPU (FP32) | 14-16 GB |
| Multi-GPU (FP32) | 10-12 GB |
| Multi-GPU (FP16) | **6-8 GB** âœ¨ |

---

## ğŸ”§ ê¸°ìˆ  ìƒì„¸

### DataParallel ë™ì‘ ë°©ì‹

```python
# 1. ëª¨ë¸ì„ ê° GPUì— ë³µì œ
model = BertForSequenceClassification(...)
model = DataParallel(model, device_ids=[0, 1, 2, 3])

# 2. Forward pass: ë°°ì¹˜ ìë™ ë¶„í• 
inputs = torch.randn(64, 128)  # Batch size = 64
outputs = model(inputs)         # GPU 0,1,2,3ì— ê°ê° 16ì”© ë¶„í• 

# 3. Backward pass: ê·¸ë˜ë””ì–¸íŠ¸ ì§‘ê³„
loss.backward()                 # ê° GPUì—ì„œ ê³„ì‚°
optimizer.step()                # Primary GPUì—ì„œ ì—…ë°ì´íŠ¸
```

### Mixed Precision Training

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# FP16 ì—°ì‚°ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, labels)

# Loss scalingìœ¼ë¡œ ì •í™•ë„ ìœ ì§€
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

## ğŸ›ï¸ ì„¤ì • ê°€ì´ë“œ

### GPU ë©”ëª¨ë¦¬ë³„ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°

| VRAM | Batch Size (GPUë‹¹) | 4 GPU Total |
|------|-------------------|-------------|
| 16 GB | 8-12 | 32-48 |
| 24 GB | 16-24 | 64-96 |
| **48 GB** | **24-32** | **96-128** |

### ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜

```python
# CPU ì½”ì–´ ìˆ˜ì˜ 50-75% ê¶Œì¥
import os
num_workers = min(8, os.cpu_count() // 2)
```

### Mixed Precision ì„¤ì •

```python
# ê¶Œì¥: í•­ìƒ í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½ + ì†ë„ í–¥ìƒ)
use_mixed_precision = True

# ì •í™•ë„ê°€ ë§¤ìš° ì¤‘ìš”í•œ ê²½ìš°ë§Œ ë¹„í™œì„±í™”
use_mixed_precision = False
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### CUDA Out of Memory

**ì¦ìƒ:**
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```

**í•´ê²°ì±…:**
```python
# 1. ë°°ì¹˜ í¬ê¸° ê°ì†Œ
trainer.prepare_data(train_texts, train_labels, batch_size=8)

# 2. Mixed Precision í™œì„±í™” (ê¸°ë³¸ê°’)
use_mixed_precision = True

# 3. GPU ìˆ˜ ê°ì†Œ
device_ids = [0, 1]  # 4ê°œ ëŒ€ì‹  2ê°œë§Œ
```

### GPU í™œìš©ë¥ ì´ ë‚®ìŒ

**ì¦ìƒ:**
```
nvidia-smi shows 20-30% GPU utilization
```

**í•´ê²°ì±…:**
```python
# 1. ì›Œì»¤ ìˆ˜ ì¦ê°€
num_workers = 8  # ê¸°ë³¸ê°’: 4

# 2. ë°°ì¹˜ í¬ê¸° ì¦ê°€
batch_size = 24  # ê¸°ë³¸ê°’: 16

# 3. Pin memory í™•ì¸ (ìë™ í™œì„±í™”ë¨)
pin_memory = True
```

### ì†ë„ í–¥ìƒì´ ê¸°ëŒ€ë³´ë‹¤ ì ìŒ

**ì ê²€ ì‚¬í•­:**

1. **GPU ê°„ ì—°ê²° í™•ì¸:**
```bash
nvidia-smi topo -m
# NVLink ì—°ê²°ì¸ì§€ í™•ì¸
```

2. **ë°°ì¹˜ í¬ê¸° í™•ì¸:**
```python
# ë„ˆë¬´ ì‘ì€ ë°°ì¹˜ëŠ” ì˜¤ë²„í—¤ë“œ ì¦ê°€
batch_size = 16  # ìµœì†Œ ê¶Œì¥ê°’
```

3. **I/O ë³‘ëª© í™•ì¸:**
```python
# ì›Œì»¤ ìˆ˜ ì¦ê°€
num_workers = 8
```

---

## ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

### ì „ì²´ ë²¤ì¹˜ë§ˆí¬

```bash
python benchmark_multigpu.py --phase all
```

### íŠ¹ì • Phaseë§Œ ë²¤ì¹˜ë§ˆí¬

```bash
# Encodingë§Œ
python benchmark_multigpu.py --phase encoding

# Trainingë§Œ
python benchmark_multigpu.py --phase training

# íŠ¹ì • GPU ì§€ì •
python benchmark_multigpu.py --phase all --gpus "0,1,2,3"
```

### ì˜ˆìƒ ì¶œë ¥

```
================================================================================
BENCHMARK SUMMARY
================================================================================
Configuration        Encoding (s)    Training (s)    Speedup
--------------------------------------------------------------------------------
Single GPU           510.25          3300.45         1.00x
2 GPUs               280.15          1850.22         1.89x
4 GPUs               155.30          1100.85         2.95x
================================================================================
```

---

## ğŸ” GPU ëª¨ë‹ˆí„°ë§

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```bash
# í„°ë¯¸ë„ 1: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python pipeline_teleclass_multigpu.py

# í„°ë¯¸ë„ 2: GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

### ìƒì„¸ ì •ë³´ í™•ì¸

```bash
# GPU ì‚¬ìš©ë¥ , ë©”ëª¨ë¦¬ ë“±
nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv

# GPU í† í´ë¡œì§€ í™•ì¸
nvidia-smi topo -m
```

---

## ğŸ“š ì½”ë“œ ì˜ˆì œ

### ì˜ˆì œ 1: ê¸°ë³¸ ì‚¬ìš©

```python
from pipeline_teleclass_multigpu import MultiGPUTELEClassPipeline

# ëª¨ë“  GPU ìë™ ì‚¬ìš©
pipeline = MultiGPUTELEClassPipeline(
    data_dir="../Amazon_products",
    output_dir="outputs",
    seed=42
)

pipeline.run()
```

### ì˜ˆì œ 2: íŠ¹ì • GPUë§Œ ì‚¬ìš©

```python
# GPU 0ê³¼ 1ë§Œ ì‚¬ìš©
pipeline = MultiGPUTELEClassPipeline(
    data_dir="../Amazon_products",
    output_dir="outputs",
    seed=42,
    device_ids=[0, 1]  # 2ê°œ GPUë§Œ
)

pipeline.run()
```

### ì˜ˆì œ 3: ë‹¨ê³„ë³„ ì‹¤í–‰

```python
from pipeline_teleclass_multigpu import (
    MultiGPUClassRepresentation,
    MultiGPUBERTTrainer,
    MultiGPUInference
)

# Phase 1: Multi-GPU Encoding
class_repr = MultiGPUClassRepresentation(device_ids=[0, 1, 2, 3])
embeddings = class_repr.encode_documents_parallel(documents, batch_size=64)

# Phase 5: Multi-GPU Training
trainer = MultiGPUBERTTrainer(
    num_classes=531,
    device_ids=[0, 1, 2, 3],
    use_mixed_precision=True
)
trainer.prepare_data(train_texts, train_labels, batch_size=16)
trainer.train(num_epochs=3)

# Phase 6: Multi-GPU Inference
inference = MultiGPUInference(
    model_path="outputs/models/best_model",
    device_ids=[0, 1, 2, 3]
)
predictions = inference.predict(test_texts, batch_size=32)
```

---

## ğŸ†š Original vs Multi-GPU ë¹„êµ

| íŠ¹ì§• | Original | Multi-GPU |
|------|----------|-----------|
| ì‹¤í–‰ ì‹œê°„ | 70-90 min | 25-35 min |
| GPU ë©”ëª¨ë¦¬ | 14-16 GB | 6-8 GB (FP16) |
| GPU ìˆ˜ | 1 | 1-4+ |
| ë°°ì¹˜ í¬ê¸° | ê³ ì • | ìë™ ìŠ¤ì¼€ì¼ë§ |
| Mixed Precision | âŒ | âœ… |
| API í˜¸í™˜ì„± | - | âœ… ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ |

### ì½”ë“œ ë¹„êµ

```python
# Original (Single GPU)
from pipeline_teleclass import TELEClassPipeline
pipeline = TELEClassPipeline(data_dir="../Amazon_products")
pipeline.run()  # ~80 min

# Multi-GPU (New)
from pipeline_teleclass_multigpu import MultiGPUTELEClassPipeline
pipeline = MultiGPUTELEClassPipeline(
    data_dir="../Amazon_products",
    device_ids=[0, 1, 2, 3]
)
pipeline.run()  # ~28 min (2.9x faster!)
```

---

## ğŸ“ ì¶”ê°€ ìë£Œ

### ë¬¸ì„œ
- **MULTIGPU_GUIDE.md**: ì¢…í•© ì‚¬ìš© ê°€ì´ë“œ (í•œê¸€)
- **MULTIGPU_SUMMARY.md**: êµ¬í˜„ ìš”ì•½ ë° ì„±ëŠ¥ ë¹„êµ
- **GPU_ARCHITECTURE.txt**: ì•„í‚¤í…ì²˜ ì‹œê°í™” (ASCII ë‹¤ì´ì–´ê·¸ë¨)

### ì°¸ê³  ë§í¬
- [PyTorch DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)
- [PyTorch Mixed Precision](https://pytorch.org/docs/stable/amp.html)
- [SentenceTransformers Multi-GPU](https://www.sbert.net/docs/training/overview.html)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹¤í–‰ ì „ í™•ì¸:
- [ ] GPU 2ê°œ ì´ìƒ ì‚¬ìš© ê°€ëŠ¥
- [ ] CUDA 11.0+ ì„¤ì¹˜ë¨
- [ ] í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¨ (`requirements_teleclass.txt`)
- [ ] ë°ì´í„° íŒŒì¼ì´ `../Amazon_products/`ì— ìˆìŒ

ì‹¤í–‰ ì¤‘ í™•ì¸:
- [ ] `nvidia-smi`ë¡œ ëª¨ë“  GPU í™œìš© ì¤‘ì¸ì§€ í™•ì¸
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ GPUë‹¹ 10GB ì´í•˜ì¸ì§€ í™•ì¸
- [ ] ë¡œê·¸ì—ì„œ "Using GPUs: [0, 1, 2, 3]" ë©”ì‹œì§€ í™•ì¸

---

## ğŸ‰ ê²°ë¡ 

Multi-GPU ë²„ì „ìœ¼ë¡œ **2.5-3ë°° ë¹ ë¥¸ ì‹¤í–‰ ì†ë„**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!

### í•µì‹¬ ì¥ì 
âœ… **ì†ë„**: 70-90ë¶„ â†’ 25-35ë¶„ (4 GPU)  
âœ… **ë©”ëª¨ë¦¬**: GPUë‹¹ 6-8GB (FP16)  
âœ… **ìë™í™”**: GPU ìë™ ê°ì§€ ë° ìµœì í™”  
âœ… **í™•ì¥ì„±**: 2-4 GPUì—ì„œ ì„ í˜• ì„±ëŠ¥ í–¥ìƒ  
âœ… **í˜¸í™˜ì„±**: ê¸°ì¡´ APIì™€ í˜¸í™˜  

### ê¶Œì¥ ì‚¬ìš©ë²•

```bash
# ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•
cd modified_teleclass
python pipeline_teleclass_multigpu.py
```

**Expected: 25-35ë¶„ ë§Œì— ì™„ë£Œ! ğŸš€**

---

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´:
1. `MULTIGPU_GUIDE.md`ì˜ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì„¹ì…˜ í™•ì¸
2. `nvidia-smi`ë¡œ GPU ìƒíƒœ í™•ì¸
3. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ ì¸¡ì •: `python benchmark_multigpu.py`

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025ë…„ 12ì›” 9ì¼  
**ë²„ì „**: 1.0  
**ìƒíƒœ**: âœ… í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ
