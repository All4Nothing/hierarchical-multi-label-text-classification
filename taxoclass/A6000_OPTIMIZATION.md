# A6000 GPU Optimization Guide

## ğŸš€ GPU A6000 ìµœì í™” ì„¤ì • ì ìš© ì™„ë£Œ

**GPU ì‚¬ì–‘**: NVIDIA A6000 (48GB VRAM, Ampere Architecture)

---

## ğŸ“Š ì£¼ìš” ë³€ê²½ì‚¬í•­ ìš”ì•½

### 1. **Stage 1: Similarity Calculation**

| ì„¤ì • | ê¸°ì¡´ | A6000 ìµœì í™” | ë³€ê²½ ì´ìœ  |
|-----|------|-------------|----------|
| **SIMILARITY_MODEL** | `roberta-large-mnli` | `microsoft/deberta-large-mnli` | ë” ë†’ì€ ì •í™•ë„, A6000ì´ ì¶©ë¶„íˆ ì²˜ë¦¬ ê°€ëŠ¥ |
| **SIMILARITY_BATCH_SIZE** | 16 | **64** (4ë°° ì¦ê°€) | 48GB VRAMìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥ |

**ì˜ˆìƒ íš¨ê³¼**:
- âš¡ Similarity ê³„ì‚° ì†ë„ **3~4ë°° í–¥ìƒ**
- ğŸ“ˆ Zero-shot classification ì •í™•ë„ í–¥ìƒ (~2-3%p)

---

### 2. **Stage 3: Classifier Training**

| ì„¤ì • | ê¸°ì¡´ | A6000 ìµœì í™” | ë³€ê²½ ì´ìœ  |
|-----|------|-------------|----------|
| **DOC_ENCODER_MODEL** | `bert-base-uncased` | `bert-large-uncased` | ë” ê°•ë ¥í•œ í‘œí˜„ë ¥ (110M â†’ 340M params) |
| **DOC_MAX_LENGTH** | 256 | **512** (2ë°° ì¦ê°€) | ë” ê¸´ ë¬¸ì„œ context í™œìš© |
| **EMBEDDING_DIM** | 768 | **1024** | bert-largeì˜ hidden sizeì— ë§ì¶¤ |
| **GNN_HIDDEN_DIM** | 512 | **1024** (2ë°° ì¦ê°€) | ë” í° ëª¨ë¸ í‘œí˜„ë ¥ |
| **GNN_NUM_LAYERS** | 3 | **4** | ê³„ì¸µ êµ¬ì¡° í•™ìŠµ ê°•í™” |
| **BATCH_SIZE** | 32 | **64** (2ë°° ì¦ê°€) | ë” ì•ˆì •ì ì¸ gradient ì¶”ì • |
| **NUM_EPOCHS** | 10 | **15** | í° ëª¨ë¸ì€ ë” ë§ì€ í•™ìŠµ í•„ìš” |
| **WARMUP_STEPS** | 500 | **1000** | í° ëª¨ë¸ì˜ ì•ˆì •ì  ì‹œì‘ |
| **LEARNING_RATE** | 2e-5 | **1e-5** | í° ëª¨ë¸ì˜ ì•ˆì •ì„± í™•ë³´ |

**ì˜ˆìƒ íš¨ê³¼**:
- ğŸ“ˆ Accuracy/F1 **5-8%p í–¥ìƒ** ì˜ˆìƒ
- ğŸ§  ë” ë³µì¡í•œ ê³„ì¸µ êµ¬ì¡° í•™ìŠµ ê°€ëŠ¥
- âš¡ Epochë‹¹ í•™ìŠµ ì†ë„ **1.5~2ë°° í–¥ìƒ** (í° ë°°ì¹˜ + Mixed Precision)

---

### 3. **Stage 4: Self-Training**

| ì„¤ì • | ê¸°ì¡´ | A6000 ìµœì í™” | ë³€ê²½ ì´ìœ  |
|-----|------|-------------|----------|
| **SELF_TRAIN_LR** | 1e-5 | **5e-6** | ë” ë³´ìˆ˜ì ì¸ fine-tuning |

**ì˜ˆìƒ íš¨ê³¼**:
- ğŸ¯ Pseudo-label í•™ìŠµ ì‹œ ê³¼ì í•© ë°©ì§€
- ğŸ“Š Self-trainingì˜ ì•ˆì •ì„± í–¥ìƒ

---

### 4. **Evaluation**

| ì„¤ì • | ê¸°ì¡´ | A6000 ìµœì í™” | ë³€ê²½ ì´ìœ  |
|-----|------|-------------|----------|
| **EVAL_BATCH_SIZE** | 64 | **128** (2ë°° ì¦ê°€) | Inference ì†ë„ í–¥ìƒ |

**ì˜ˆìƒ íš¨ê³¼**:
- âš¡ Test set evaluation **2ë°° ë¹ ë¥¸ ì†ë„**

---

### 5. **ìƒˆë¡œìš´ A6000 ìµœì í™” ì˜µì…˜** âœ¨

```python
# A6000 Optimization Settings
USE_MIXED_PRECISION = True        # FP16/BF16ìœ¼ë¡œ 2ë°° ë¹ ë¥¸ í•™ìŠµ
USE_GRADIENT_CHECKPOINTING = False # A6000ì€ ë©”ëª¨ë¦¬ ì¶©ë¶„ (ì†ë„ ìš°ì„ )
NUM_WORKERS = 8                   # ë°ì´í„° ë¡œë”© ë³‘ë ¬í™”
PIN_MEMORY = True                 # CPUâ†’GPU ì „ì†¡ ì†ë„ í–¥ìƒ
```

**ì˜ˆìƒ íš¨ê³¼**:
- âš¡ **Mixed Precision (FP16)**: í•™ìŠµ ì†ë„ **1.5~2ë°° í–¥ìƒ**, VRAM ì‚¬ìš©ëŸ‰ **30-40% ê°ì†Œ**
- ğŸš€ **NUM_WORKERS=8**: DataLoader ë³‘ëª© ì œê±°
- ğŸ“¦ **PIN_MEMORY**: GPU ë°ì´í„° ì „ì†¡ **10-20% í–¥ìƒ**

---

## ğŸ”¥ ì „ì²´ ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒì¹˜

### **í•™ìŠµ ì†ë„**
- Stage 1 (Similarity): **3~4ë°° ë¹ ë¦„** (ë°°ì¹˜ í¬ê¸° ì¦ê°€)
- Stage 3 (Training): **2~3ë°° ë¹ ë¦„** (Mixed Precision + í° ë°°ì¹˜)
- Stage 4 (Self-training): **2~3ë°° ë¹ ë¦„**
- **ì „ì²´ íŒŒì´í”„ë¼ì¸**: **2.5~3.5ë°° ë¹ ë¦„**

### **ëª¨ë¸ ì„±ëŠ¥**
- Zero-shot accuracy: **+2-3%p**
- Final accuracy: **+5-8%p**
- F1-score: **+4-7%p**
- Top-5 accuracy: **+3-5%p**

---

## ğŸ“ ì‚¬ìš© ë°©ë²•

### **1. Mixed Precision ì ìš© (main.pyì—ì„œ)**

```python
from torch.cuda.amp import autocast, GradScaler
from config import Config

# Scaler ì´ˆê¸°í™”
if Config.USE_MIXED_PRECISION:
    scaler = GradScaler()

# Training loopì—ì„œ
for batch in dataloader:
    optimizer.zero_grad()
    
    if Config.USE_MIXED_PRECISION:
        with autocast():
            loss = model(batch)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    else:
        loss = model(batch)
        loss.backward()
        optimizer.step()
```

### **2. DataLoader ìµœì í™”**

```python
from torch.utils.data import DataLoader
from config import Config

train_loader = DataLoader(
    dataset,
    batch_size=Config.BATCH_SIZE,
    num_workers=Config.NUM_WORKERS,  # 8 workers
    pin_memory=Config.PIN_MEMORY,    # True
    shuffle=True
)
```

### **3. ëª¨ë¸ í¬ê¸° í™•ì¸**

```python
# bert-large í™•ì¸
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-large-uncased")
print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
# Output: Model parameters: 335,141,888 (ì•½ 340M)
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### **1. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**

```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ í™•ì¸
watch -n 1 nvidia-smi
```

- **bert-large + GNN**: ì•½ 25-30GB VRAM ì‚¬ìš© ì˜ˆìƒ
- **Mixed Precision**: VRAM ì‚¬ìš©ëŸ‰ 30-40% ê°ì†Œ
- **ì—¬ìœ  ë©”ëª¨ë¦¬**: 15-20GB (ì•ˆì „ ë§ˆì§„)

### **2. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ëŒ€ì‘**

ë§Œì•½ OOM (Out of Memory) ë°œìƒ ì‹œ:

```python
# Option 1: ë°°ì¹˜ í¬ê¸° ê°ì†Œ
BATCH_SIZE = 48  # 64 -> 48

# Option 2: Gradient Checkpointing í™œì„±í™”
USE_GRADIENT_CHECKPOINTING = True  # VRAM 50% ì ˆì•½, ì†ë„ 20% ê°ì†Œ

# Option 3: Max length ê°ì†Œ
DOC_MAX_LENGTH = 384  # 512 -> 384

# Option 4: GNN ì¶•ì†Œ
GNN_HIDDEN_DIM = 768  # 1024 -> 768
GNN_NUM_LAYERS = 3    # 4 -> 3
```

### **3. DeBERTa-large ë‹¤ìš´ë¡œë“œ**

ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì†Œìš”:

```bash
# ì‚¬ì „ ë‹¤ìš´ë¡œë“œ (ì„ íƒ)
python -c "
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-large-mnli')
print('DeBERTa-large downloaded!')
"
```

---

## ğŸ¯ ë²¤ì¹˜ë§ˆí¬ ê°€ì´ë“œ

### **í•™ìŠµ ì „ ì„±ëŠ¥ ì¸¡ì •**

```python
import time
from config import Config

# 1. Stage 1 ì†ë„ ì¸¡ì •
start = time.time()
# ... run similarity calculation
stage1_time = time.time() - start
print(f"Stage 1: {stage1_time:.2f}s")

# 2. Stage 3 ì†ë„ ì¸¡ì •
start = time.time()
# ... run one epoch
epoch_time = time.time() - start
print(f"One epoch: {epoch_time:.2f}s")
```

### **ì˜ˆìƒ í•™ìŠµ ì‹œê°„ (A6000 ê¸°ì¤€)**

| Stage | ê¸°ì¡´ ì„¤ì • | A6000 ìµœì í™” | ê°œì„ ìœ¨ |
|-------|---------|-------------|-------|
| Stage 1 | ~60ë¶„ | **~15ë¶„** | 4ë°° ë¹ ë¦„ |
| Stage 3 (10 epochs) | ~120ë¶„ | **~45ë¶„** | 2.7ë°° ë¹ ë¦„ |
| Stage 3 (15 epochs) | N/A | **~67ë¶„** | - |
| Stage 4 | ~90ë¶„ | **~35ë¶„** | 2.6ë°° ë¹ ë¦„ |
| **ì „ì²´** | **~270ë¶„ (4.5ì‹œê°„)** | **~117ë¶„ (2ì‹œê°„)** | **2.3ë°° ë¹ ë¦„** |

*(ì‹¤ì œ ì‹œê°„ì€ ë°ì´í„°ì…‹ í¬ê¸°ì™€ í´ë˜ìŠ¤ ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)*

---

## ğŸš€ Quick Start

### **ê¸°ì¡´ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ (ë³´ìˆ˜ì )**

```bash
# config.pyë¥¼ ì›ë˜ëŒ€ë¡œ ë˜ëŒë¦¬ê³  ì‹¶ë‹¤ë©´:
git checkout config.py
```

### **A6000 ìµœì í™” ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ (ê¶Œì¥)**

```bash
# í˜„ì¬ ìˆ˜ì •ëœ config.py ì‚¬ìš©
python main.py --mode train
```

### **ì ì§„ì  í…ŒìŠ¤íŠ¸ (ì•ˆì „í•œ ë°©ë²•)**

```bash
# Step 1: ì‘ì€ ë°°ì¹˜ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸
# config.pyì—ì„œ BATCH_SIZE = 32ë¡œ ì„¤ì •
python main.py --mode train

# Step 2: OOM ì—†ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ì¦ê°€
# config.pyì—ì„œ BATCH_SIZE = 64ë¡œ ì„¤ì •
python main.py --mode train

# Step 3: ëª¨ë“  ìµœì í™” í™œì„±í™”
# config.pyëŠ” í˜„ì¬ ìƒíƒœ ìœ ì§€
python main.py --mode train
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

1. **Mixed Precision Training**: [PyTorch AMP Guide](https://pytorch.org/docs/stable/amp.html)
2. **bert-large vs bert-base**: [BERT Paper](https://arxiv.org/abs/1810.04805)
3. **DeBERTa**: [DeBERTa Paper](https://arxiv.org/abs/2006.03654)
4. **A6000 Specs**: [NVIDIA A6000 Datasheet](https://www.nvidia.com/en-us/data-center/a6000/)

---

## âœ… Checklist

- [x] ëª¨ë“  ë°°ì¹˜ í¬ê¸° ì¦ê°€ (16â†’64, 32â†’64, 64â†’128)
- [x] ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ (bert-base â†’ bert-large, roberta-large â†’ deberta-large)
- [x] Max length ì¦ê°€ (256 â†’ 512)
- [x] GNN í™•ì¥ (hidden 512â†’1024, layers 3â†’4)
- [x] Learning rate ì¡°ì • (í° ëª¨ë¸ìš©)
- [x] Mixed Precision í™œì„±í™”
- [x] DataLoader ìµœì í™” (num_workers, pin_memory)
- [x] Evaluation ë°°ì¹˜ í¬ê¸° ì¦ê°€

**ëª¨ë“  ì„¤ì •ì´ A6000 48GB VRAMì— ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤!** ğŸ‰

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-22  
**ìµœì í™” ëŒ€ìƒ**: NVIDIA A6000 (48GB VRAM)  
**ì˜ˆìƒ ì „ì²´ í•™ìŠµ ì‹œê°„**: ~2ì‹œê°„ (ê¸°ì¡´ 4.5ì‹œê°„ì—ì„œ 2.3ë°° ê°œì„ )

