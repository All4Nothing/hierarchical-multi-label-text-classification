# TaxoClass: Hierarchical Multi-Label Text Classification

PyTorch implementation of **TaxoClass** framework from the paper:  
*"TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names"*

## üìã Overview

TaxoClass is a weakly-supervised framework for hierarchical multi-label text classification that only requires:
- Document texts
- Class names (no labeled training data)
- Taxonomy hierarchy structure

### Key Features

‚úÖ **Four-Stage Pipeline:**
1. **Document-Class Similarity**: Using textual entailment (RoBERTa-MNLI)
2. **Core Class Mining**: Top-down candidate selection with confidence scoring
3. **Classifier Training**: BERT + GNN architecture
4. **Self-Training**: Multi-label self-training with KL divergence

‚úÖ **Hierarchy-Aware**: Graph Neural Network encodes taxonomy structure  
‚úÖ **No Labeled Data Required**: Weakly-supervised learning from class names  
‚úÖ **Flexible**: Supports any hierarchical taxonomy

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TaxoClass Pipeline                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Stage 1: Document-Class Similarity (RoBERTa-MNLI)        ‚îÇ
‚îÇ  Stage 2: Core Class Mining (Top-down + Confidence)       ‚îÇ
‚îÇ  Stage 3: Classifier Training (BERT + GNN)                ‚îÇ
‚îÇ  Stage 4: Self-Training (KL Divergence)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure

```
taxoclass/
‚îú‚îÄ‚îÄ config.py                  # Configuration settings
‚îú‚îÄ‚îÄ main.py                    # Main pipeline
‚îú‚îÄ‚îÄ requirements.txt           # Dependencies
‚îú‚îÄ‚îÄ README.md                  # This file
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ loader.py              # Data loading and preprocessing
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ similarity.py          # Stage 1: Similarity calculation
‚îÇ   ‚îú‚îÄ‚îÄ core_mining.py         # Stage 2: Core class mining
‚îÇ   ‚îú‚îÄ‚îÄ classifier.py          # Stage 3: Classifier (BERT+GNN)
‚îÇ   ‚îî‚îÄ‚îÄ self_training.py       # Stage 4: Self-training
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ hierarchy.py           # Taxonomy hierarchy processing
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py             # Evaluation metrics
‚îÇ
‚îú‚îÄ‚îÄ cache/                     # Cached similarity matrices
‚îú‚îÄ‚îÄ outputs/                   # Output files and metrics
‚îî‚îÄ‚îÄ saved_models/              # Trained model checkpoints
```

## üöÄ Installation

### Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA (optional, for GPU acceleration)

### Install Dependencies

```bash
pip install -r requirements.txt
```

Required packages:
- torch>=2.0.0
- transformers>=4.30.0
- torch-geometric>=2.3.0
- scikit-learn>=1.3.0
- numpy>=1.24.0
- pandas>=2.0.0
- tqdm>=4.65.0
- networkx>=3.0

## üìä Data Format

### Input Files

Your data directory should contain:

1. **classes.txt**: Class ID and name mapping
```
0    grocery_gourmet_food
1    meat_poultry
2    jerky
...
```

2. **class_hierarchy.txt**: Parent-child relationships
```
0    1
0    8
1    2
...
```

3. **train/train_corpus.txt**: Training documents
```
0    document text here...
1    another document...
...
```

4. **test/test_corpus.txt**: Test documents (same format)

### Update Configuration

Edit `config.py` to point to your data directory:

```python
DATA_DIR = "../Amazon_products"
```

## üéØ Usage

### Basic Usage

Run the complete TaxoClass pipeline:

```bash
python main.py
```

This will execute all four stages and evaluate on test data.

### Advanced Usage

#### Using Different Similarity Models

```python
# In main.py, modify Stage 1:

# Option 1: Fast similarity (sentence transformers)
use_fast_similarity = True

# Option 2: Full NLI model (more accurate)
use_fast_similarity = False
```

#### Adjusting Training Parameters

Edit `config.py`:

```python
# Classifier training
LEARNING_RATE = 2e-5
BATCH_SIZE = 32
NUM_EPOCHS = 10

# Self-training
SELF_TRAIN_ITERATIONS = 5
SELF_TRAIN_TEMPERATURE = 2.0
SELF_TRAIN_THRESHOLD = 0.5
```

#### Skip Self-Training

```python
# In main.py
run_self_training = False
```

### Stage-by-Stage Execution

You can run individual stages:

```python
from config import Config
from utils.hierarchy import TaxonomyHierarchy
from data.loader import DocumentCorpus
from models.similarity import DocumentClassSimilarity

# Stage 1: Similarity calculation
hierarchy = TaxonomyHierarchy(Config.HIERARCHY_FILE, Config.CLASSES_FILE)
corpus = DocumentCorpus(Config.TRAIN_CORPUS)

similarity_calculator = DocumentClassSimilarity(device="cuda")
similarity_matrix = similarity_calculator.compute_similarity_matrix(
    documents=corpus.get_all_texts(),
    class_names=hierarchy.id_to_name
)
```

## üìà Evaluation Metrics

TaxoClass reports multiple evaluation metrics:

### Standard Metrics
- **Micro-F1 / Macro-F1**: Standard classification metrics
- **Precision / Recall**: At different thresholds
- **Hamming Loss**: Multi-label classification loss

### Hierarchical Metrics
- **Hierarchical Precision@k**: Considers ancestor classes
- **Hierarchical Recall@k**: Considers ancestor classes
- **Hierarchical F1@k**: Harmonic mean
- **nDCG@k**: Normalized Discounted Cumulative Gain

### Example Output

```
==============================================================
Evaluation Metrics
==============================================================

F1 Scores:
  Micro-F1: 0.6523
  Macro-F1: 0.5847

Precision & Recall:
  Micro-Precision: 0.6891
  Macro-Precision: 0.6234
  Micro-Recall: 0.6189
  Macro-Recall: 0.5512

Hamming Loss: 0.0234

Hierarchical Metrics:

  Top-5:
    H-Precision: 0.7234
    H-Recall: 0.6812
    H-F1: 0.7015
    nDCG: 0.7456
==============================================================
```

## üîß Configuration Options

Key configuration parameters in `config.py`:

### Stage 1: Similarity
```python
SIMILARITY_MODEL = "roberta-large-mnli"
SIMILARITY_BATCH_SIZE = 16
HYPOTHESIS_TEMPLATE = "This document is about {class_name}"
```

### Stage 2: Core Mining
```python
CANDIDATE_SELECTION_POWER = 2  # (level+1)^2
CONFIDENCE_THRESHOLD_PERCENTILE = 50  # Median
```

### Stage 3: Classifier
```python
DOC_ENCODER_MODEL = "bert-base-uncased"
EMBEDDING_DIM = 768
GNN_HIDDEN_DIM = 512
GNN_NUM_LAYERS = 3
```

### Stage 4: Self-Training
```python
SELF_TRAIN_ITERATIONS = 5
SELF_TRAIN_TEMPERATURE = 2.0
SELF_TRAIN_THRESHOLD = 0.5
```

## üí° Tips & Best Practices

### GPU Memory Management

If you encounter OOM errors:

```python
# Reduce batch size
BATCH_SIZE = 16
SIMILARITY_BATCH_SIZE = 8

# Use gradient accumulation
# (implement in trainer if needed)
```

### Improve Performance

1. **Use full NLI model** (Stage 1): More accurate but slower
2. **Increase GNN layers** (Stage 3): Better hierarchy encoding
3. **More self-training iterations** (Stage 4): Better convergence

### Speed Up Training

1. **Use fast similarity** (Stage 1): Sentence transformers
2. **Reduce training epochs** (Stage 3)
3. **Skip self-training** for quick experiments

## üêõ Troubleshooting

### Common Issues

**Issue**: `CUDA out of memory`  
**Solution**: Reduce batch sizes in `config.py`

**Issue**: Similarity calculation is slow  
**Solution**: Set `use_fast_similarity = True` in `main.py`

**Issue**: Poor performance on small datasets  
**Solution**: Reduce model complexity or use pretrained class embeddings

## üìö Citation

If you use this code, please cite the original paper:

```bibtex
@inproceedings{taxoclass,
  title={TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names},
  author={...},
  booktitle={...},
  year={2023}
}
```

## üìù License

This implementation is for educational and research purposes.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## üìß Contact

For questions or issues, please open a GitHub issue.

---

**Note**: This is an implementation for the TaxoClass framework. Adjust hyperparameters based on your specific dataset and task requirements.

