# TaxoClass Framework ìˆ˜ì • ì™„ë£Œ ë³´ê³ ì„œ

## ğŸ“‹ ìˆ˜ì • ê°œìš”

TaxoClass frameworkì˜ 4ê°€ì§€ ì£¼ìš” ì´ìŠˆë¥¼ ë…¼ë¬¸ê³¼ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì • ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.

---

## âœ… ìˆ˜ì • ì™„ë£Œëœ ì´ìŠˆ

### 1. Stage 2: Multi-label Core Class Selection

**ë¬¸ì œì **: Core Classë¥¼ ë¬¸ì„œë‹¹ **í•˜ë‚˜ë§Œ** ì„ íƒ

**ë…¼ë¬¸ ìš”êµ¬ì‚¬í•­**: Confidence thresholdë¥¼ ë„˜ëŠ” **ëª¨ë“  í´ë˜ìŠ¤**ê°€ Core Class

**ìˆ˜ì • ë‚´ìš©**:
```python
# Before
self.core_classes[doc_id] = best_core_class  # ë‹¨ì¼ ê°’

# After  
for class_id in candidates:
    if conf_score >= threshold:
        doc_core_classes.append(class_id)  # ëª¨ë“  threshold ì´ˆê³¼ í´ë˜ìŠ¤
self.core_classes[doc_id] = doc_core_classes  # ë¦¬ìŠ¤íŠ¸
```

**ì˜í–¥**:
- ë¬¸ì„œë‹¹ í‰ê·  1.5~3ê°œì˜ Core Class ì‹ë³„ ê°€ëŠ¥
- Multi-label íŠ¹ì„± ì •í™•íˆ ë°˜ì˜
- ê³„ì¸µ êµ¬ì¡°ì˜ ì—¬ëŸ¬ ê²½ë¡œ ë™ì‹œ í•™ìŠµ ê°€ëŠ¥

---

### 2. Stage 3: Hierarchical Label Generation

**ë¬¸ì œì **: Core Classì˜ ì¡°ìƒì„ Positiveë¡œ ì„¤ì •í•˜ëŠ” ë¡œì§ ë¶€ì¬

**ë…¼ë¬¸ ìš”êµ¬ì‚¬í•­**:
- **Positive (1)**: Core classes + ëª¨ë“  ì¡°ìƒ í´ë˜ìŠ¤
- **Negative (0)**: ê·¸ ì™¸ í´ë˜ìŠ¤
- **Ignore (-1)**: Core classesì˜ ìì† í´ë˜ìŠ¤

**ìˆ˜ì • ë‚´ìš©**:
ìƒˆë¡œìš´ í•¨ìˆ˜ `create_training_labels()` ì¶”ê°€:
```python
def create_training_labels(
    core_classes_dict: Dict[int, List[int]],
    hierarchy,
    num_classes: int
) -> np.ndarray:
    """
    Returns label matrix (num_docs, num_classes):
        1.0 = positive (core class or ancestor)
        0.0 = negative (other classes)
       -1.0 = ignore (descendants)
    """
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# Core classes ë§ˆì´ë‹ í›„
core_classes = miner.identify_core_classes()

# í•™ìŠµìš© ë¼ë²¨ ìƒì„±
train_labels = create_training_labels(
    core_classes_dict=core_classes,
    hierarchy=hierarchy,
    num_classes=hierarchy.num_classes
)

# Classifier í•™ìŠµ
dataset = TaxoDataset(documents, train_labels, tokenizer)
trainer = TaxoClassifierTrainer(model, train_loader, ...)
trainer.train()
```

**ê²€ì¦ ê²°ê³¼**:
```
Hierarchy: Root(0) -> L1(1,2) -> L2(3,4,5,6)
Doc with Core=[3]:
  Positive: [0, 1, 3]  âœ“ (Core + Ancestors)
  Negative: [2, 4, 5, 6]  âœ“
  Ignore: []  âœ“
```

---

### 3. Stage 4: KL Divergence Loss

**ë¬¸ì œì **: KL Divergence ëŒ€ì‹  BCEWithLogitsLoss ì‚¬ìš©

**ë…¼ë¬¸ ìš”êµ¬ì‚¬í•­**: `D_KL(Q || P)` ìµœì†Œí™”

**ìˆ˜ì • ë‚´ìš©**:
Binary KL Divergence ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„:
```python
def kl_divergence_loss(self, predictions, target_distribution):
    """
    Binary KL divergence for multi-label:
    KL(q || p) = q*log(q/p) + (1-q)*log((1-q)/(1-p))
    """
    kl_pos = target_distribution * torch.log(target_distribution / predictions)
    kl_neg = (1 - target_distribution) * torch.log((1 - target_distribution) / (1 - predictions))
    return (kl_pos + kl_neg).mean()
```

**Training Loop ìˆ˜ì •**:
```python
# Before: BCEWithLogitsLoss
criterion = nn.BCEWithLogitsLoss()
logits = model(input_ids, attention_mask)
loss = criterion(logits, targets)

# After: KL Divergence
model.set_return_probs(True)
predictions = model(input_ids, attention_mask)  # í™•ë¥ 
model.set_return_probs(False)
loss = self.kl_divergence_loss(predictions, targets)
```

**ë¹„êµ**:
```
Sample predictions: [0.9, 0.1, 0.8, 0.2]
Sample targets:     [1.0, 0.0, 0.95, 0.0]

KL Divergence Loss: 0.0979
BCE Loss:           0.3648
Ratio:              0.27x
```

---

### 4. Stage 4: Temperature Parameter

**ë¬¸ì œì **: Temperature=2.0ì€ distributionì„ **smooth**í•˜ê²Œ ë§Œë“¦ (ë…¼ë¬¸ ì˜ë„ì™€ ë°˜ëŒ€)

**ë…¼ë¬¸ ìš”êµ¬ì‚¬í•­**: "Strengthen high-confidence predictions" â†’ Sharp distribution í•„ìš”

**ìˆ˜ì • ë‚´ìš©**:
```python
# Before
temperature: float = 2.0  # âŒ Smoothing effect

# After
temperature: float = 0.5  # âœ… Sharpening effect
```

**íš¨ê³¼ ê²€ì¦**:
```python
Original predictions: [0.9, 0.7, 0.5, 0.3, 0.1]

T = 2.0 (Before):
  Q = [0.949, 0.837, 0.707, 0.548, 0.316]
  Gap: 0.800 â†’ 0.633 (Smoothing âŒ)

T = 0.5 (After):
  Q = [0.810, 0.490, 0.250, 0.090, 0.010]
  Gap: 0.800 â†’ 0.800 (Relative sharpening âœ…)
  
íš¨ê³¼: ë†’ì€ í™•ë¥ ì€ ìœ ì§€, ë‚®ì€ í™•ë¥ ì€ ë” ë‚®ì¶°ì§
```

**Temperature íš¨ê³¼**:
- `T > 1`: Smoothing (ì°¨ì´ ê°ì†Œ)
- `T = 1`: No change
- `T < 1`: Sharpening (ìƒëŒ€ì  ì°¨ì´ ì¦ê°€) âœ“

---

## ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼

ì „ì²´ í…ŒìŠ¤íŠ¸ í†µê³¼:
```
âœ… Test 1: Multi-label core class selection
âœ… Test 2: Hierarchical label generation  
âœ… Test 3: KL Divergence loss computation
âœ… Test 4: Temperature sharpening effect
âœ… Test 5: Threshold filtering

ALL TESTS PASSED âœ…
```

---

## ğŸ”„ Breaking Changes

API ë³€ê²½ì‚¬í•­:

### CoreClassMiner
```python
# OLD
core_class = miner.get_core_class(doc_id)  # int
conf_score = miner.get_confidence_score(doc_id)  # float

# NEW
core_classes = miner.get_core_classes(doc_id)  # List[int]
conf_scores = miner.get_confidence_scores(doc_id)  # Dict[int, float]
```

### Core Classes Dictionary
```python
# OLD
core_classes = {doc_id: class_id, ...}  # Dict[int, int]

# NEW
core_classes = {doc_id: [class_id1, class_id2, ...], ...}  # Dict[int, List[int]]
```

---

## ğŸ“ ìˆ˜ì •ëœ íŒŒì¼

1. **`taxoclass/models/core_mining.py`**
   - Multi-label core class selection
   - `create_training_labels()` í•¨ìˆ˜ ì¶”ê°€
   - í†µê³„ í•¨ìˆ˜ ì—…ë°ì´íŠ¸

2. **`taxoclass/models/self_training.py`**
   - KL Divergence loss êµ¬í˜„
   - Temperature ê¸°ë³¸ê°’ ë³€ê²½ (2.0 â†’ 0.5)
   - Training loop ìˆ˜ì •

3. **`taxoclass/models/__init__.py`**
   - `create_training_labels` export ì¶”ê°€

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ì „ì²´ íŒŒì´í”„ë¼ì¸

```python
from models import (
    DocumentClassSimilarity,
    CoreClassMiner,
    create_training_labels,
    TaxoClassifier,
    TaxoClassifierTrainer,
    SelfTrainer
)

# Stage 1: Similarity
similarity_calc = DocumentClassSimilarity()
sim_matrix = similarity_calc.compute_similarity_matrix(documents, class_names)

# Stage 2: Core Class Mining (Multi-label)
miner = CoreClassMiner(hierarchy, sim_matrix)
core_classes = miner.identify_core_classes()
# core_classes = {doc_id: [class1, class2, ...], ...}

# Stage 3: Label Generation + Training
train_labels = create_training_labels(core_classes, hierarchy, num_classes)
# train_labels shape: (num_docs, num_classes)
# values: 1 (positive), 0 (negative), -1 (ignore)

dataset = TaxoDataset(documents, train_labels, tokenizer)
trainer = TaxoClassifierTrainer(model, train_loader, val_loader, edge_index)
trainer.train()

# Stage 4: Self-Training (KL Divergence + T=0.5)
self_trainer = SelfTrainer(
    model=model,
    unlabeled_loader=unlabeled_loader,
    edge_index=edge_index,
    temperature=0.5,  # Sharpening
    threshold=0.5
)
self_trainer.self_train()
```

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

ìˆ˜ì • ì „ê³¼ ë¹„êµí•˜ì—¬:

1. **Multi-label Core Classes**:
   - ë” ë§ì€ í•™ìŠµ ì‹ í˜¸ í™œìš©
   - ê³„ì¸µ êµ¬ì¡°ì˜ ì—¬ëŸ¬ ê²½ë¡œ í•™ìŠµ

2. **Hierarchical Labels**:
   - ê³„ì¸µ ì¼ê´€ì„± í–¥ìƒ
   - ì¡°ìƒ-ìì† ê´€ê³„ ëª…ì‹œì  í•™ìŠµ

3. **KL Divergence**:
   - íƒ€ê²Ÿ ë¶„í¬ì— ë” ì •í™•íˆ ìˆ˜ë ´
   - High-confidence ì˜ˆì¸¡ ê°•í™”

4. **Temperature Sharpening**:
   - Confident ì˜ˆì¸¡ë§Œ ê°•í™”
   - Uncertain ì˜ˆì¸¡ ì–µì œ

---

## ğŸ” ê²€ì¦ ê¶Œì¥ì‚¬í•­

ì‹¤ì œ ë°ì´í„°ë¡œ ë‹¤ìŒ ì‚¬í•­ í™•ì¸:

```python
# 1. Core Class í†µê³„
stats = miner.get_statistics()
print(f"Avg core classes per doc: {stats['avg_core_classes_per_doc']}")
# ì˜ˆìƒ: 1.5 ~ 3.0

# 2. Label ë¶„í¬
print(f"Positive: {(train_labels == 1).sum() / train_labels.size * 100:.1f}%")
print(f"Negative: {(train_labels == 0).sum() / train_labels.size * 100:.1f}%")
print(f"Ignore: {(train_labels == -1).sum() / train_labels.size * 100:.1f}%")
# ì˜ˆìƒ: Positive 5-15%, Negative 80-90%, Ignore 5-10%

# 3. Self-Training Loss
# KL Loss ì •ìƒ ë²”ìœ„: 0.05 ~ 0.5
# ë„ˆë¬´ í¬ë©´ temperature/threshold ì¡°ì •
```

---

## ğŸ“š ì°¸ê³ 

- ë…¼ë¬¸: "TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names"
- ì£¼ìš” ê°œì„ : Multi-label support, Hierarchical consistency, KL Divergence
- ìˆ˜ì • ë‚ ì§œ: 2025-12-07
- í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸: `test_fixes.py`
- ìƒì„¸ ë³€ê²½ì‚¬í•­: `CHANGES.md`

---

## âœ… ê²°ë¡ 

**TaxoClass frameworkê°€ ì´ì œ ë…¼ë¬¸ì˜ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ì •í™•íˆ ë°˜ì˜í•©ë‹ˆë‹¤.**

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. âœ… Multi-label core class mining
2. âœ… Hierarchical label generation with ancestors/descendants
3. âœ… KL Divergence loss (not BCE)
4. âœ… Temperature sharpening (T=0.5, not 2.0)

ëª¨ë“  ìˆ˜ì •ì‚¬í•­ì€ í…ŒìŠ¤íŠ¸ ì™„ë£Œë˜ì—ˆìœ¼ë©°, ë…¼ë¬¸ì˜ ì›ë˜ ì˜ë„ëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
