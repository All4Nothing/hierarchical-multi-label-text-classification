# Inference Methods Guide

## ğŸ¯ ë‘ ê°€ì§€ Inference ë°©ì‹

TaxoClass frameworkëŠ” ì´ì œ ë‘ ê°€ì§€ inference ë°©ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤:

### Method 1: Leaf-centric + Post-processing (ê¸°ë³¸)
- êµ¬ì²´ì ì¸ leaf ë…¸ë“œ ë³´ì¥
- ë‹¨ì¼ ê³„ì¸µ ê²½ë¡œ ì„ íƒ
- ì‹¤ìš©ì , Kaggle ë“±ì— ì í•©

### Method 2: Pure Threshold + Ancestor Closure (ë…¼ë¬¸ ë°©ì‹)
- ë…¼ë¬¸ì— ì¶©ì‹¤í•œ êµ¬í˜„
- í™•ë¥  ìš°ì„  ì„ íƒ
- Multi-label ì™„ì „ í™œìš©

---

## ğŸ“Š Sample 1 ì˜ˆì¸¡ ì°¨ì´ ë¶„ì„

### ëª¨ë¸ ì˜ˆì¸¡
```python
Top-5 predictions:
  Class 3:  0.9999647  (Level 1)
  Class 17: 0.99920005 (Level 2)
  Class 28: 0.9982666  (Level 2)
  Class 34: 0.9858106  (Level 2)
  Class 4:  0.9663014  (Level 1)
```

### Method 1 ê²°ê³¼: `"15,17,56"`
**ê³¼ì •**:
1. Threshold 0.3 â†’ ë§ì€ í´ë˜ìŠ¤ ì„ íƒ
2. Best leaf ì„ íƒ (ì˜ˆ: 56)
3. 56ì˜ ê²½ë¡œ: [10, 15, 17, 56]
4. ê¹Šì€ 3ê°œ: [15, 17, 56]

**íŠ¹ì§•**:
- âœ… Leaf ë³´ì¥ (56)
- âŒ ë†’ì€ í™•ë¥  í´ë˜ìŠ¤ 3, 28, 34 ëˆ„ë½

### Method 2 ê²°ê³¼: `"3,17,28"` (ì˜ˆìƒ)
**ê³¼ì •**:
1. Threshold 0.3 â†’ ë§ì€ í´ë˜ìŠ¤ ì„ íƒ
2. ì¡°ìƒ ì¶”ê°€
3. í™•ë¥  ìƒìœ„ 3ê°œ: [3, 17, 28]

**íŠ¹ì§•**:
- âœ… ìµœê³  í™•ë¥  ë°˜ì˜
- âš ï¸ Leaf ë¯¸ë³´ì¥

---

## ğŸš€ ì‹¤í–‰ ëª…ë ¹ì–´

### 1. Leaf-centric (ê¸°ë³¸, í˜„ì¬ êµ¬í˜„)
```bash
python generate_submission.py \
    --threshold 0.5 \
    --min_labels 2 \
    --max_labels 3 \
    --output submission_leafcentric.csv
```

**ì–¸ì œ ì‚¬ìš©**:
- êµ¬ì²´ì  ì¹´í…Œê³ ë¦¬ê°€ ì¤‘ìš”
- Leaf ì˜ˆì¸¡ì´ í•„ìˆ˜
- Kaggle ë“± ê²½ìŸ

---

### 2. Pure Threshold (ë…¼ë¬¸ ë°©ì‹)
```bash
python generate_submission.py \
    --threshold 0.5 \
    --min_labels 2 \
    --max_labels 3 \
    --pure_threshold \
    --output submission_pure.csv
```

**ì–¸ì œ ì‚¬ìš©**:
- ë…¼ë¬¸ ì¬í˜„/ë¹„êµ
- ëª¨ë¸ í™•ì‹  ìµœëŒ€ í™œìš©
- ë‹¤ì–‘í•œ ë ˆë²¨ í˜¼í•© í•„ìš”

---

### 3. Threshold ë¹„êµ ì‹¤í—˜
```bash
# Leaf-centric with different thresholds
for t in 0.3 0.4 0.5 0.6 0.7; do
    python generate_submission.py \
        --threshold $t \
        --output "results/leafcentric_t${t}.csv"
done

# Pure threshold with different thresholds
for t in 0.3 0.4 0.5 0.6 0.7; do
    python generate_submission.py \
        --threshold $t \
        --pure_threshold \
        --output "results/pure_t${t}.csv"
done
```

---

### 4. ë‘ ë°©ì‹ ë¹„êµ
```bash
# Generate submissions with both methods
python generate_submission.py \
    --threshold 0.5 \
    --output submission_method1.csv

python generate_submission.py \
    --threshold 0.5 \
    --pure_threshold \
    --output submission_method2.csv

# Compare results
python compare_methods.py \
    --file1 submission_method1.csv \
    --file2 submission_method2.csv
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
SAMPLE-BY-SAMPLE COMPARISON
================================================================================

Sample 1:
  Method 1: [15, 17, 56]
  Method 2: [3, 17, 28]
  Common:   [17]
  Only in Method 1: [15, 56]
  Only in Method 2: [3, 28]

Sample 2:
  Method 1: [10, 64, 338]
  Method 2: [10, 64, 338]
  Common:   [10, 64, 338]

...

âš ï¸  5/10 samples differ (50.0%)
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê°€ì´ë“œ

### 1. ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€
```bash
# If you have validation labels
python evaluate_submission.py \
    --predictions submission_method1.csv \
    --ground_truth validation_labels.csv \
    --output eval_method1.txt

python evaluate_submission.py \
    --predictions submission_method2.csv \
    --ground_truth validation_labels.csv \
    --output eval_method2.txt
```

### 2. ë¹„êµ ë©”íŠ¸ë¦­
- **Precision**: ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì •ë‹µ ë¹„ìœ¨
- **Recall**: ì •ë‹µ ì¤‘ ì˜ˆì¸¡í•œ ë¹„ìœ¨
- **F1 Score**: Harmonic mean
- **Hierarchical metrics**: Path-based evaluation

### 3. ì˜ˆìƒ ì°¨ì´
| Metric | Leaf-centric | Pure Threshold |
|--------|--------------|----------------|
| Precision | Medium-High | High |
| Recall | Medium | Medium-High |
| Leaf Coverage | 100% | Varies |
| High-Prob Match | Low | High |

---

## ğŸ” ë””ë²„ê¹… & ë¶„ì„

### íŠ¹ì • ìƒ˜í”Œ ë¶„ì„
```bash
# Compare specific samples
python compare_methods.py \
    --file1 submission_method1.csv \
    --file2 submission_method2.csv \
    --samples 1,5,10,20,50
```

### ì „ì²´ ë¶„ì„
```bash
python compare_methods.py \
    --file1 submission_method1.csv \
    --file2 submission_method2.csv \
    --all
```

### ì˜ˆì¸¡ í™•ë¥  í™•ì¸
```python
# In generate_submission.py, uncomment debug output:
for i in range(min(5, len(predictions))):
    top_5 = np.argsort(predictions[i])[-5:][::-1]
    top_5_probs = predictions[i][top_5]
    print(f"Sample {i} top-5: {top_5.tolist()}, probs: {top_5_probs}")
```

---

## âš–ï¸ ì„ íƒ ê°€ì´ë“œ

### Use Leaf-centric if:
```
âœ“ ë°ì´í„°ì…‹ì´ ëª…í™•í•œ leaf ì¹´í…Œê³ ë¦¬ ìš”êµ¬
âœ“ êµ¬ì²´ì  ì˜ˆì¸¡ì´ ì¤‘ìš” (e.g., ìƒí’ˆ ë¶„ë¥˜)
âœ“ Kaggle ë“± ê²½ìŸ
âœ“ ê³„ì¸µ ê²½ë¡œê°€ ì¤‘ìš”
```

### Use Pure Threshold if:
```
âœ“ ë…¼ë¬¸ ì¬í˜„ì´ ëª©ì 
âœ“ ëª¨ë¸ í™•ì‹ ì„ ìµœëŒ€í•œ í™œìš©
âœ“ ë‹¤ì–‘í•œ ì¶”ìƒí™” ë ˆë²¨ í•„ìš”
âœ“ Multi-label íŠ¹ì„± ì™„ì „ í™œìš©
```

### ì¶”ì²œ ì›Œí¬í”Œë¡œìš°
```bash
# 1. Both methods with default threshold
python generate_submission.py --threshold 0.5 --output m1_t05.csv
python generate_submission.py --threshold 0.5 --pure_threshold --output m2_t05.csv

# 2. Compare
python compare_methods.py --file1 m1_t05.csv --file2 m2_t05.csv

# 3. Tune threshold for best method
for t in 0.3 0.4 0.5 0.6 0.7; do
    python generate_submission.py \
        --threshold $t \
        --pure_threshold \  # or remove for leaf-centric
        --output "tuning/t${t}.csv"
done

# 4. Evaluate on validation set (if available)
# Choose best threshold

# 5. Final submission with best config
python generate_submission.py \
    --threshold 0.5 \
    --pure_threshold \
    --output final_submission.csv
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- **PREDICTION_ANALYSIS.md** - ì›ì¸ ë¶„ì„ ìƒì„¸
- **METHOD_COMPARISON.md** - ë‘ ë°©ì‹ ì‹¬ì¸µ ë¹„êµ
- **INFERENCE_ANALYSIS.md** - ì „ì²´ inference ì „ëµ ë¶„ì„
- **RUN_COMMANDS.md** - ëª¨ë“  ì‹¤í–‰ ëª…ë ¹ì–´

---

## â“ FAQ

### Q1: ì–´ëŠ ë°©ì‹ì´ ë” ë‚˜ì€ê°€ìš”?
**A**: ë°ì´í„°ì…‹ê³¼ ëª©ì ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤. ê²€ì¦ ë°ì´í„°ë¡œ ë‘ ë°©ì‹ ëª¨ë‘ ì‹¤í—˜í•´ë³´ì„¸ìš”.

### Q2: Sample 1ì´ ì™œ [15,17,56]ì´ ë˜ì—ˆë‚˜ìš”?
**A**: Leaf-centric ë°©ì‹ì´ leaf 56ì„ ìš°ì„  ì„ íƒí•˜ê³  ê·¸ ê²½ë¡œë§Œ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ìƒì„¸í•œ ë¶„ì„ì€ `PREDICTION_ANALYSIS.md` ì°¸ì¡°.

### Q3: Pure thresholdê°€ ë…¼ë¬¸ì— ë” ì¶©ì‹¤í•œê°€ìš”?
**A**: ë„¤. ë…¼ë¬¸ì€ threshold-based multi-label classificationì„ ëª…ì‹œí•˜ë©°, explicit path selectionì€ ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### Q4: ë‘ ë°©ì‹ì„ ê²°í•©í•  ìˆ˜ ìˆë‚˜ìš”?
**A**: ë„¤. Ensemble ë°©ì‹ìœ¼ë¡œ ë‘ ê²°ê³¼ë¥¼ ê²°í•©í•˜ê±°ë‚˜, votingì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Q5: Thresholdë¥¼ ì–´ë–»ê²Œ ì„ íƒí•˜ë‚˜ìš”?
**A**: 0.3~0.7 ë²”ìœ„ì—ì„œ ì‹¤í—˜í•˜ì—¬ ê²€ì¦ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ê°’ì„ ì„ íƒí•˜ì„¸ìš”.

---

## ğŸ¯ ë¹ ë¥¸ ì‹œì‘

```bash
# 1. ë‘ ë°©ì‹ ë¹„êµ
python generate_submission.py --threshold 0.5 --output method1.csv
python generate_submission.py --threshold 0.5 --pure_threshold --output method2.csv

# 2. ì°¨ì´ í™•ì¸
python compare_methods.py --file1 method1.csv --file2 method2.csv

# 3. ìµœì  ë°©ì‹ ì„ íƒ
# (ê²€ì¦ ë°ì´í„°ë¡œ í‰ê°€ í›„)

# 4. ìµœì¢… ì œì¶œ
python generate_submission.py \
    --threshold 0.5 \
    --pure_threshold \
    --output final_submission.csv
```

ì´ì œ ë‘ ê°€ì§€ ë°©ì‹ì„ ììœ ë¡­ê²Œ ì‹¤í—˜í•˜ê³  ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ‰
