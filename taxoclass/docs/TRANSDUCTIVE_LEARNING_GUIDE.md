# Transductive Learning Strategy Guide

## ğŸ¯ Train + Test Data í™œìš© ì „ëµ

ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” **Train dataì™€ Test data ëª¨ë‘ ë¼ë²¨ì´ ì—†ëŠ” ìƒí™©**ì—ì„œ, ë‘ ë°ì´í„°ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

---

## ğŸ“š ë°°ê²½: Inductive vs Transductive Learning

### **Inductive Learning (ì „í†µì  ë°©ë²•)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Train  â”‚â”€â”€â”€â”€â”€â”€â†’â”‚  Model  â”‚â”€â”€â”€â”€â”€â”€â†’â”‚   Test  â”‚
â”‚ (seen)  â”‚ Learn â”‚ (learn) â”‚ Pred. â”‚ (unseen)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

íŠ¹ì§•:
- Test setì€ í•™ìŠµ ì¤‘ ì „í˜€ ë³´ì§€ ì•ŠìŒ
- ì¼ë°˜í™”(Generalization)ì— ì§‘ì¤‘
- ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ëŠ¥ë ¥ ì¤‘ìš”
```

### **Transductive Learning (ì´ í”„ë¡œì íŠ¸)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Train + Test    â”‚â”€â”€â”€â”€â”€â”€â†’â”‚  Model  â”‚â”€â”€â”€â”€â”€â”€â†’â”‚   Test  â”‚
â”‚ (all seen, no Y) â”‚ Learn â”‚ (learn) â”‚ Pred. â”‚  (seen) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

íŠ¹ì§•:
- Test setë„ í•™ìŠµ ì¤‘ í™œìš© (ë‹¨, ë¼ë²¨ ì—†ì´)
- Test set ë¶„í¬ì— ëŒ€í•œ ì ì‘(Adaptation)
- ë” ë§ì€ unlabeled data í™œìš©
```

---

## âœ… **ì´ ì „ëµì´ ì í•©í•œ ì´ìœ **

### **1. ë¬¸ì œ ì„¤ì •ì´ Transductive**

```python
# ì£¼ì–´ì§„ ì¡°ê±´:
# - Train corpus: ë¼ë²¨ ì—†ìŒ (unlabeled)
# - Test corpus: ë¼ë²¨ ì—†ìŒ (unlabeled)
# - ëª©í‘œ: Test corpusì˜ ë¬¸ì„œë“¤ì„ í´ë˜ìŠ¤ì— í• ë‹¹

# â†’ Test setì´ ê³ ì •ë˜ì–´ ìˆê³ , ë¯¸ë¦¬ ì•Œ ìˆ˜ ìˆìŒ
# â†’ Transductive settingì— ì™„ë²½íˆ ë¶€í•©
```

### **2. Semi-Supervised Learningì— ìœ ë¦¬**

```python
# Stage 1-2: Zero-shot + Core Class Mining
# â†’ Pseudo-labels ìƒì„± (ìë™ìœ¼ë¡œ ë¼ë²¨ ì¶”ì •)

# Trainë§Œ ì‚¬ìš©:
#   - Pseudo-labeled samples: ~10,000
#   - Model robustness: Medium

# Train + Test ì‚¬ìš©:
#   - Pseudo-labeled samples: ~30,000
#   - Model robustness: High
#   - Test distribution í•™ìŠµ: âœ…
```

### **3. ê³„ì¸µ êµ¬ì¡° í•™ìŠµì— íš¨ê³¼ì **

```python
# Hierarchical classification:
# - Level 0 (Root) â†’ Level 1 â†’ ... â†’ Level N (Leaf)

# GNN (Graph Neural Network) ì‚¬ìš©:
# - ë” ë§ì€ ë¬¸ì„œ = ë” í’ë¶€í•œ class-document ê´€ê³„
# - Test data í¬í•¨ ì‹œ ê³„ì¸µ êµ¬ì¡° í•™ìŠµ ê°•í™”

# ì˜ˆì‹œ:
# Train: "laptop" â†’ Electronics > Computers
# Test: "gaming laptop" â†’ ê°™ì€ ê²½ë¡œ ê°•í™”
#   â†’ GNNì´ ë” í™•ì‹  ìˆê²Œ í•™ìŠµ
```

---

## âš ï¸ **ì£¼ì˜ì‚¬í•­ & ë¦¬ìŠ¤í¬**

### **1. Data Leakage ë°©ì§€**

```python
âŒ ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ:

# 1. Test label ì‚¬ìš© (ë‹¹ì—°íˆ ì•ˆë¨)
if test_labels:  # âŒ
    model.fit(test_data, test_labels)

# 2. Test statisticsë¥¼ hyperparameter tuningì— ì‚¬ìš©
best_threshold = optimize_on_test_accuracy()  # âŒ

# 3. Test-specific feature engineering
if doc in test_set:  # âŒ
    features = special_transform(doc)

âœ… ì˜¬ë°”ë¥¸ ì‚¬ìš©:

# 1. Test dataë¥¼ unlabeled dataë¡œ ì·¨ê¸‰
unlabeled_data = train_data + test_data
model.fit_semi_supervised(unlabeled_data)

# 2. ëª¨ë“  ë°ì´í„°ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
for doc in all_data:
    features = transform(doc)  # ë™ì¼í•œ ì²˜ë¦¬

# 3. Confidence-based pseudo-labeling
pseudo_labels = model.predict_with_confidence(unlabeled_data)
confident_samples = filter_by_threshold(pseudo_labels)
```

### **2. Overfitting to Test Distribution**

```python
# ë¦¬ìŠ¤í¬:
# Test setì˜ íŠ¹ì´í•œ ë¶„í¬ì— ê³¼ì í•©ë  ìˆ˜ ìˆìŒ

# ì˜ˆì‹œ:
# Train: "electronics", "books", "clothing" (ê· ë“± ë¶„í¬)
# Test: "electronics" (90%), "books" (10%)
#   â†’ ëª¨ë¸ì´ "electronics"ì— ê³¼ë„í•˜ê²Œ í¸í–¥ë  ìˆ˜ ìˆìŒ

# í•´ê²°ì±…:
# 1. Regularization ê°•í™”
DROPOUT = 0.15        # â†‘ ì¦ê°€
WEIGHT_DECAY = 0.02   # â†‘ ì¦ê°€

# 2. Confidence threshold ë³´ìˆ˜ì  ì„¤ì •
SELF_TRAIN_THRESHOLD = 0.6  # ë†’ê²Œ ì„¤ì •

# 3. Class balance ê³ ë ¤
use_class_weights = True
```

### **3. Generalization í•œê³„**

```python
# Train + Testë¡œ í•™ìŠµí•œ ëª¨ë¸:
# âœ… ì´ Test setì— ìµœì í™”
# âš ï¸ ìƒˆë¡œìš´ unseen dataì—ëŠ” ì¼ë°˜í™” ì œí•œ

# ì‹œë‚˜ë¦¬ì˜¤:
# 1. í˜„ì¬ Test set ì˜ˆì¸¡: âœ… ìµœê³  ì„±ëŠ¥
# 2. ë‹¤ìŒ ë‹¬ ìƒˆ ë°ì´í„°: âš ï¸ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥
# 3. ë‹¤ë¥¸ ë„ë©”ì¸ ë°ì´í„°: âš ï¸ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥

# ëŒ€ì‘:
# - ìƒˆ ë°ì´í„° ì¶”ê°€ ì‹œ: Re-training ë˜ëŠ” Fine-tuning
# - ëª¨ë¸ ì¬ì‚¬ìš© ì‹œ: Train-only ë²„ì „ë„ ë³´ê´€
```

---

## ğŸ¯ **ë‹¨ê³„ë³„ ì‚¬ìš© ì „ëµ (ê¶Œì¥)** â­

### **í˜„ì¬ Config ì„¤ì •**

```python
# config.pyì— ì¶”ê°€ëœ ì˜µì…˜:
USE_TEST_IN_STAGE1 = True   # âœ… Zero-shot (ì•ˆì „)
USE_TEST_IN_STAGE2 = True   # âœ… Mining (ì•ˆì „)
USE_TEST_IN_STAGE3 = False  # âš ï¸ Training (ë³´ìˆ˜ì )
USE_TEST_IN_STAGE4 = True   # âœ… Self-training (ì ì§„ì )
```

### **Stageë³„ ìƒì„¸ ì „ëµ**

#### **Stage 1: Zero-shot Classification** âœ…

```python
USE_TEST_IN_STAGE1 = True  # ê¶Œì¥: True

ì´ìœ :
- Zero-shotì€ ë¼ë²¨ì„ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- NLI ëª¨ë¸ë¡œ similarityë§Œ ê³„ì‚°
- Test data ì‚¬ìš©í•´ë„ leakage ì—†ìŒ
- ë” ë§ì€ ë¬¸ì„œë¡œ class-document similarity íŒŒì•…

íš¨ê³¼:
- Test set ë¶„í¬ íŒŒì•…
- ëª¨ë“  classì— ëŒ€í•œ ì „ë°˜ì  ì´í•´
```

#### **Stage 2: Core Class Mining** âœ…

```python
USE_TEST_IN_STAGE2 = True  # ê¶Œì¥: True

ì´ìœ :
- Confidence ê¸°ë°˜ ìƒ˜í”Œ ì„ íƒ
- ë†’ì€ í™•ì‹ ë„ë¥¼ ê°€ì§„ ìƒ˜í”Œë§Œ ì‚¬ìš©
- Pseudo-label í’ˆì§ˆì´ ë†’ìŒ
- Test data í¬í•¨ ì‹œ ë” ë§ì€ training samples

íš¨ê³¼:
- Training samples ì¦ê°€ (ì˜ˆ: 10k â†’ 30k)
- ë‹¤ì–‘í•œ classì— ëŒ€í•œ examples
- Class imbalance ì™„í™”
```

#### **Stage 3: Initial Classifier Training** âš ï¸

```python
USE_TEST_IN_STAGE3 = False  # ê¶Œì¥: False (ë³´ìˆ˜ì )

ì´ìœ :
- ì´ˆê¸° ëª¨ë¸ì€ ë³´ìˆ˜ì ìœ¼ë¡œ í•™ìŠµ
- Train dataë§Œìœ¼ë¡œ ê²¬ê³ í•œ baseline êµ¬ì¶•
- Overfitting ë°©ì§€
- Validation ì„±ëŠ¥ìœ¼ë¡œ hyperparameter tuning

ëŒ€ì•ˆ (ê³µê²©ì ):
- USE_TEST_IN_STAGE3 = True
- ë‹¨, Regularization ê°•í™” í•„ìš”
- Dropout: 0.15, Weight Decay: 0.02
```

#### **Stage 4: Self-Training** âœ…

```python
USE_TEST_IN_STAGE4 = True  # ê¶Œì¥: True

ì´ìœ :
- Pseudo-labelë¡œ ì ì§„ì  í•™ìŠµ
- Confidence thresholdë¡œ í’ˆì§ˆ ê´€ë¦¬
- Test distributionì— ì ì‘
- Unlabeled dataì˜ ìµœëŒ€ í™œìš©

íš¨ê³¼:
- Test setì— ëŒ€í•œ ì„±ëŠ¥ ìµœëŒ€í™”
- Confident samplesë¶€í„° ì ì§„ì  í™•ì¥
- Model confidence í–¥ìƒ
```

---

## ğŸ“Š **ì‹¤í—˜ ë¹„êµ í”„ë¡œí† ì½œ**

### **ì‹¤í—˜ ì„¤ì •**

```python
# Experiment 1: Train-only (Inductive)
USE_TEST_IN_STAGE1 = False
USE_TEST_IN_STAGE2 = False
USE_TEST_IN_STAGE3 = False
USE_TEST_IN_STAGE4 = False

# Experiment 2: Gradual (Recommended)
USE_TEST_IN_STAGE1 = True
USE_TEST_IN_STAGE2 = True
USE_TEST_IN_STAGE3 = False
USE_TEST_IN_STAGE4 = True

# Experiment 3: Aggressive (Maximum performance)
USE_TEST_IN_STAGE1 = True
USE_TEST_IN_STAGE2 = True
USE_TEST_IN_STAGE3 = True
USE_TEST_IN_STAGE4 = True
```

### **í‰ê°€ ì§€í‘œ**

```python
# 1. Test set ì„±ëŠ¥ (ì£¼ ëª©í‘œ)
- Accuracy@1, @3, @5, @10
- Macro/Micro F1-score
- Per-level accuracy

# 2. í•™ìŠµ ì•ˆì •ì„±
- Training loss curve
- Validation loss (if using train/val split)
- Pseudo-label quality over iterations

# 3. íš¨ìœ¨ì„±
- Total training time
- Number of pseudo-labeled samples
- Convergence speed
```

### **ì˜ˆìƒ ê²°ê³¼**

```python
# Test Accuracy (ì˜ˆìƒ)
Train-only:     75-78%
Gradual:        80-83%  â­ (ê¶Œì¥)
Aggressive:     82-85%  (overfitting risk)

# Generalization (unseen data)
Train-only:     Good     (70-75%)
Gradual:        Fair     (68-73%)
Aggressive:     Poor     (65-70%)

# Training Time
Train-only:     Baseline
Gradual:        +10-20%  (ë” ë§ì€ ë°ì´í„°)
Aggressive:     +20-30%
```

---

## ğŸ’¡ **ì‹¤ì „ íŒ**

### **1. ì ì§„ì  ë„ì… (Safest)**

```python
# Step 1: Baseline (Train-only)
python main.py --mode train  # with all False

# Step 2: Add Stage 1-2
USE_TEST_IN_STAGE1 = True
USE_TEST_IN_STAGE2 = True
python main.py --mode train

# Step 3: Add Stage 4
USE_TEST_IN_STAGE4 = True
python main.py --mode train

# Step 4: Compare results
# â†’ ì„±ëŠ¥ í–¥ìƒ í™•ì¸ í›„ ìµœì¢… ê²°ì •
```

### **2. Regularization íŠœë‹**

```python
# Test data ì‚¬ìš© ì‹œ Regularization ê°•í™”:

# Dropout ì¦ê°€
GNN_DROPOUT = 0.15  # 0.1 â†’ 0.15

# Weight decay ì¦ê°€
WEIGHT_DECAY = 0.02  # 0.01 â†’ 0.02

# Confidence threshold ìƒí–¥
SELF_TRAIN_THRESHOLD = 0.6  # 0.5 â†’ 0.6

# Temperature ì¡°ì •
SELF_TRAIN_TEMPERATURE = 2.5  # 2.0 â†’ 2.5 (smoother)
```

### **3. Pseudo-label í’ˆì§ˆ ëª¨ë‹ˆí„°ë§**

```python
# Self-training ì¤‘ ë¡œê¹…:
def log_pseudo_label_quality(pseudo_labels, confidence_scores):
    # 1. Confidence distribution
    print(f"Mean confidence: {confidence_scores.mean():.3f}")
    print(f"Std confidence: {confidence_scores.std():.3f}")
    
    # 2. Class distribution
    class_counts = Counter(pseudo_labels)
    print(f"Class distribution: {class_counts}")
    
    # 3. High confidence ratio
    high_conf = (confidence_scores > 0.8).mean()
    print(f"High confidence ratio: {high_conf:.3f}")

# Warning signs:
# - Mean confidence < 0.5: ëª¨ë¸ì´ ë¶ˆí™•ì‹¤
# - Std confidence > 0.3: ì¼ê´€ì„± ë¶€ì¡±
# - Class imbalance > 10:1: í¸í–¥ ìœ„í—˜
```

### **4. Early Stopping with Validation Split**

```python
# Train dataë¥¼ train/valë¡œ ë¶„ë¦¬ (optional)
from sklearn.model_selection import train_test_split

train_docs, val_docs = train_test_split(
    train_data, 
    test_size=0.1,  # 10% validation
    random_state=42
)

# Val setìœ¼ë¡œ early stopping
best_val_loss = float('inf')
patience = 3
patience_counter = 0

for epoch in range(num_epochs):
    train_loss = train_one_epoch(train_docs)
    val_loss = evaluate(val_docs)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        save_model()
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping!")
            break

# ìµœì¢…: train + val + testë¡œ ì¬í•™ìŠµ (optional)
final_train_data = train_docs + val_docs + test_docs
final_model = retrain(final_train_data)
```

---

## ğŸ”¬ **ì´ë¡ ì  ë°°ê²½**

### **Transductive Learningì˜ ìˆ˜í•™ì  ì •ì˜**

```
Inductive Learning:
  Given: D_train = {(x_i, y_i)}_{i=1}^{n}
  Learn: f: X â†’ Y
  Goal: Minimize E_{(x,y)~P}[L(f(x), y)]
  
Transductive Learning:
  Given: D_train = {(x_i, y_i)}_{i=1}^{n}, X_test = {x_j}_{j=1}^{m}
  Learn: f: X â†’ Y (with knowledge of X_test)
  Goal: Minimize Î£_{j=1}^{m} L(f(x_j), y_j)
  
ì°¨ì´ì :
- Inductive: ë¯¸ë˜ì˜ ëª¨ë“  xì— ëŒ€í•´ ì¼ë°˜í™”
- Transductive: ì£¼ì–´ì§„ X_testì— ëŒ€í•´ ìµœì í™”
```

### **Semi-Supervised Learningê³¼ì˜ ê´€ê³„**

```python
# Semi-supervised learning:
# - Labeled data: small
# - Unlabeled data: large
# - Goal: Use unlabeled data to improve model

# ì´ í”„ë¡œì íŠ¸:
# - Labeled data: 0 (pseudo-labelsë¡œ ìƒì„±)
# - Unlabeled data: train + test (both large)
# - Goal: Generate pseudo-labels and learn

# ìœ ì‚¬í•œ ê¸°ë²•:
# - Pseudo-labeling
# - Self-training
# - Co-training
# - Consistency regularization
```

---

## ğŸ“ˆ **ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸**

### **Before Training**
- [ ] Data distribution ë¶„ì„ (train vs test)
- [ ] Class distribution í™•ì¸
- [ ] Regularization ì„¤ì • í™•ì¸
- [ ] Validation strategy ê²°ì •

### **During Training**
- [ ] Pseudo-label í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- [ ] Confidence distribution ë¡œê¹…
- [ ] Loss curve í™•ì¸ (overfitting ì—¬ë¶€)
- [ ] Class balance ëª¨ë‹ˆí„°ë§

### **After Training**
- [ ] Test set ì„±ëŠ¥ í‰ê°€
- [ ] Per-class ì„±ëŠ¥ ë¶„ì„
- [ ] Confidence analysis
- [ ] Error analysis (misclassified samples)

---

## ğŸ“ **ê²°ë¡  & ê¶Œì¥ì‚¬í•­**

### **âœ… ê¶Œì¥: Gradual Approach (ë‹¨ê³„ë³„ ì‚¬ìš©)**

```python
# config.py ì„¤ì • (í˜„ì¬ ì ìš©ë¨):
USE_TEST_IN_STAGE1 = True   # âœ…
USE_TEST_IN_STAGE2 = True   # âœ…
USE_TEST_IN_STAGE3 = False  # âš ï¸ (ë³´ìˆ˜ì )
USE_TEST_IN_STAGE4 = True   # âœ…
```

**ì´ìœ **:
1. âœ… ì•ˆì „ì„±: Overfitting ìœ„í—˜ ìµœì†Œí™”
2. âœ… ì„±ëŠ¥: Test setì— ëŒ€í•´ ë†’ì€ ì„±ëŠ¥
3. âœ… íˆ¬ëª…ì„±: ê° ë‹¨ê³„ë³„ ê¸°ì—¬ë„ íŒŒì•… ê°€ëŠ¥
4. âœ… ìœ ì—°ì„±: í•„ìš”ì‹œ Stage 3ë„ ì¶”ê°€ ê°€ëŠ¥

**ì˜ˆìƒ ì„±ëŠ¥**:
- Test Accuracy: **80-83%**
- Training Time: Baseline + 10-20%
- Robustness: High
- Generalization: Fair

---

## ğŸ“š **ì°¸ê³  ë¬¸í—Œ**

1. **Transductive Learning**:
   - Vapnik, V. (1998). Statistical Learning Theory
   - Joachims, T. (1999). Transductive Inference for Text Classification

2. **Semi-Supervised Learning**:
   - Zhu, X. & Goldberg, A. B. (2009). Introduction to Semi-Supervised Learning
   - Chapelle et al. (2006). Semi-Supervised Learning

3. **Self-Training**:
   - Yarowsky, D. (1995). Unsupervised Word Sense Disambiguation
   - Lee, D. H. (2013). Pseudo-Label: The Simple and Efficient Method

4. **Hierarchical Classification**:
   - Silla, C. N. & Freitas, A. A. (2011). A Survey of Hierarchical Classification
   - Kowsari et al. (2019). Text Classification Algorithms

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-22  
**ì „ëµ**: Transductive Learning with Gradual Test Data Integration  
**ì˜ˆìƒ Test Accuracy**: 80-83%

