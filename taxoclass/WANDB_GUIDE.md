# Weights & Biases (wandb) Integration Guide

## ğŸ¯ **Wandbë¥¼ í†µí•œ í•™ìŠµ ëª¨ë‹ˆí„°ë§**

ì´ í”„ë¡œì íŠ¸ëŠ” Weights & Biases (wandb)ë¥¼ í†µí•´ ì „ì²´ í•™ìŠµ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

---

## ğŸ“¦ **ì„¤ì¹˜**

### **1. wandb ì„¤ì¹˜**

```bash
pip install wandb
```

ë˜ëŠ” requirements.txtë¥¼ í†µí•´ ì„¤ì¹˜:

```bash
pip install -r requirements.txt
```

### **2. wandb ë¡œê·¸ì¸**

```bash
wandb login
```

- ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©´ wandb ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸
- API í‚¤ë¥¼ ë³µì‚¬í•˜ì—¬ í„°ë¯¸ë„ì— ë¶™ì—¬ë„£ê¸°
- ë˜ëŠ” [https://wandb.ai/authorize](https://wandb.ai/authorize)ì—ì„œ ì§ì ‘ API í‚¤ í™•ì¸

---

## ğŸš€ **ì‚¬ìš© ë°©ë²•**

### **Option 1: config.pyì—ì„œ í™œì„±í™” (ê¶Œì¥)**

```python
# config.py
USE_WANDB = True  # wandb ì‚¬ìš© í™œì„±í™”
WANDB_PROJECT = "taxoclass-hierarchical"  # í”„ë¡œì íŠ¸ ì´ë¦„
WANDB_ENTITY = None  # íŒ€ ì´ë¦„ (ê°œì¸ ê³„ì •ì€ None)
WANDB_RUN_NAME = None  # Run ì´ë¦„ (ìë™ ìƒì„±ë¨)
WANDB_TAGS = ["hierarchical", "taxonomy", "gnn"]  # íƒœê·¸
```

```bash
# ê·¸ëŒ€ë¡œ ì‹¤í–‰
python main.py
```

### **Option 2: ì½”ë“œ ì‹¤í–‰ ì‹œ ì œì–´**

```python
# config.pyì—ì„œ USE_WANDB = Falseë¡œ ì„¤ì •
USE_WANDB = False

# ì‹¤í–‰ ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´
export USE_WANDB=1  # ë˜ëŠ”
WANDB_MODE=offline python main.py  # ì˜¤í”„ë¼ì¸ ëª¨ë“œ
```

### **Option 3: wandb ì—†ì´ ì‹¤í–‰**

```bash
# wandbë¥¼ ì„¤ì¹˜í•˜ì§€ ì•Šê±°ë‚˜ USE_WANDB=False
python main.py
# â†’ ìë™ìœ¼ë¡œ wandb ì—†ì´ ì‹¤í–‰ë¨ (ê²½ê³  ë©”ì‹œì§€ë§Œ ì¶œë ¥)
```

---

## ğŸ“Š **ë¡œê¹…ë˜ëŠ” ë©”íŠ¸ë¦­**

### **1. Stage 1: Similarity Calculation**

```python
stage1/similarity_min      # Similarity ìµœì†Œê°’
stage1/similarity_max      # Similarity ìµœëŒ€ê°’
stage1/similarity_mean     # Similarity í‰ê· 
stage1/similarity_std      # Similarity í‘œì¤€í¸ì°¨
```

**ë¶„ì„**:
- Similarity ë¶„í¬ë¥¼ í†µí•´ zero-shot classification í’ˆì§ˆ íŒŒì•…
- Meanì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ëª¨ë¸ê³¼ ë°ì´í„° ê°„ mismatch
- Stdê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ëª¨ë“  í´ë˜ìŠ¤ê°€ ë¹„ìŠ·í•˜ê²Œ ë³´ì„

---

### **2. Stage 2: Core Class Mining**

```python
stage2/num_core_classes           # Core class ê°œìˆ˜
stage2/total_docs_with_core       # Coreê°€ í• ë‹¹ëœ ë¬¸ì„œ ìˆ˜
stage2/avg_docs_per_core_class    # Core classë‹¹ í‰ê·  ë¬¸ì„œ ìˆ˜
```

**ë¶„ì„**:
- Core class ê°œìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´: Threshold ë„ˆë¬´ ë‚®ìŒ
- Core class ê°œìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´: Threshold ë„ˆë¬´ ë†’ìŒ
- ì´ìƒì : ì „ì²´ classì˜ 20-30%ê°€ core class

---

### **3. Stage 3: Classifier Training**

#### **ë°ì´í„° í†µê³„**
```python
stage3/num_positive_labels    # Positive label ê°œìˆ˜
stage3/num_negative_labels    # Negative label ê°œìˆ˜
stage3/num_ignored_labels     # Ignored label (-1) ê°œìˆ˜
stage3/positive_ratio         # Positive label ë¹„ìœ¨
stage3/train_samples          # Training sample ìˆ˜
stage3/val_samples            # Validation sample ìˆ˜
```

#### **í•™ìŠµ ê³¼ì • (ì‹¤ì‹œê°„)**
```python
stage3/train_loss             # Training loss (ë§¤ 10 step)
stage3/learning_rate          # Learning rate (ë§¤ 10 step)
stage3/epoch                  # í˜„ì¬ epoch
```

#### **Epochë³„ ë©”íŠ¸ë¦­**
```python
stage3/epoch_train_loss       # Epoch í‰ê·  train loss
stage3/epoch_val_loss         # Epoch í‰ê·  validation loss
stage3/best_val_loss          # ìµœê³  validation loss
stage3/best_epoch             # ìµœê³  ì„±ëŠ¥ epoch
```

**ë¶„ì„**:
- Train loss vs Val loss: Overfitting ì—¬ë¶€ í™•ì¸
- Learning rate schedule: Warmup í›„ ê°ì†Œ í™•ì¸
- Best epoch: Early stoppingì´ ì ì ˆí•œì§€ í™•ì¸

---

### **4. Stage 4: Self-Training**

#### **Iterationë³„ í†µê³„**
```python
stage4/confident_predictions  # Confident sample ìˆ˜
stage4/confidence_ratio       # Confidence ë¹„ìœ¨ (%)
stage4/avg_max_prediction     # í‰ê·  ìµœëŒ€ ì˜ˆì¸¡ê°’
stage4/avg_target_entropy     # í‰ê·  target entropy
stage4/iteration              # í˜„ì¬ iteration
```

#### **í•™ìŠµ ê³¼ì •**
```python
stage4/self_train_loss        # Self-training loss
stage4/epoch                  # Epoch within iteration
```

**ë¶„ì„**:
- Confident predictions ì¦ê°€ ì¶”ì„¸: ëª¨ë¸ì´ í™•ì‹  ì¦ê°€
- Avg max prediction ì¦ê°€: Pseudo-label í’ˆì§ˆ í–¥ìƒ
- Avg target entropy ê°ì†Œ: ë” sharpí•œ ì˜ˆì¸¡

---

### **5. Test (Final Evaluation)**

```python
test/accuracy                 # Test accuracy
test/precision                # Precision
test/recall                   # Recall
test/f1_score                 # F1 score
test/top3_accuracy            # Top-3 accuracy
test/top5_accuracy            # Top-5 accuracy
test/top10_accuracy           # Top-10 accuracy
test/level_0_accuracy         # Level 0 accuracy
test/level_1_accuracy         # Level 1 accuracy
...
```

**ë¶„ì„**:
- Overall accuracy: ìµœì¢… ì„±ëŠ¥
- Top-k accuracy: Ranking í’ˆì§ˆ
- Level-wise accuracy: ê³„ì¸µë³„ ì„±ëŠ¥

---

## ğŸ“ˆ **Wandb Dashboard í™œìš©**

### **1. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**

```
Run Page â†’ Charts íƒ­
- Train loss curve
- Validation loss curve
- Learning rate schedule
- Confidence ratio over iterations
```

**ìœ ìš©í•œ ì°¨íŠ¸**:

#### **Loss Curves (Stage 3)**
```python
# X-axis: global_step
# Y-axis: stage3/train_loss, stage3/epoch_val_loss
# â†’ Overfitting ê°ì§€
```

#### **Self-Training Progress (Stage 4)**
```python
# X-axis: iteration
# Y-axis: stage4/confidence_ratio, stage4/avg_max_prediction
# â†’ Pseudo-label í’ˆì§ˆ ê°œì„  í™•ì¸
```

---

### **2. ì‹¤í—˜ ë¹„êµ**

```
Project Page â†’ Runs íƒ­
- ì—¬ëŸ¬ runì„ ì„ íƒí•˜ì—¬ ë¹„êµ
- Parallel coordinates plot
- Scatter plot matrix
```

**ë¹„êµ ì˜ˆì‹œ**:

```python
# ì‹¤í—˜ 1: bert-base + GNN 3-layer
# ì‹¤í—˜ 2: bert-large + GNN 4-layer
# ì‹¤í—˜ 3: bert-large + GNN 4-layer + Test data in Stage 3

# ë¹„êµ ë©”íŠ¸ë¦­:
# - test/accuracy
# - stage3/best_val_loss
# - stage4/confidence_ratio
# - Training time
```

---

### **3. Hyperparameter Tuning**

```
Sweeps íƒ­ â†’ Create sweep
```

**ê¶Œì¥ sweep ì„¤ì •**:

```yaml
# sweep.yaml
program: main.py
method: bayes  # or grid, random
metric:
  name: test/accuracy
  goal: maximize
parameters:
  learning_rate:
    min: 5e-6
    max: 5e-5
  gnn_hidden_dim:
    values: [512, 768, 1024]
  gnn_num_layers:
    values: [3, 4, 5]
  self_train_threshold:
    min: 0.4
    max: 0.7
```

```bash
# Sweep ì‹œì‘
wandb sweep sweep.yaml
wandb agent <sweep_id>
```

---

## ğŸ¨ **Custom Visualizations**

### **1. Confusion Matrix (ì¶”ê°€ ê°€ëŠ¥)**

```python
# main.pyì— ì¶”ê°€
if use_wandb:
    # Compute confusion matrix
    wandb.log({
        "test/confusion_matrix": wandb.plot.confusion_matrix(
            probs=None,
            y_true=true_labels,
            preds=pred_labels,
            class_names=class_names
        )
    })
```

### **2. Class-wise Performance (ì¶”ê°€ ê°€ëŠ¥)**

```python
# main.pyì— ì¶”ê°€
if use_wandb:
    # Create table
    table = wandb.Table(
        columns=["Class", "Precision", "Recall", "F1", "Support"],
        data=class_metrics
    )
    wandb.log({"test/class_performance": table})
```

### **3. Prediction Examples (ì¶”ê°€ ê°€ëŠ¥)**

```python
# main.pyì— ì¶”ê°€
if use_wandb:
    # Log sample predictions
    examples = []
    for i in range(10):
        examples.append([
            test_documents[i],
            true_labels[i],
            pred_labels[i],
            "âœ“" if true_labels[i] == pred_labels[i] else "âœ—"
        ])
    
    table = wandb.Table(
        columns=["Document", "True", "Predicted", "Correct"],
        data=examples
    )
    wandb.log({"test/prediction_examples": table})
```

---

## ğŸ”§ **ê³ ê¸‰ ì„¤ì •**

### **1. Offline Mode (ë„¤íŠ¸ì›Œí¬ ì—†ì´)**

```bash
# ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ì‹¤í–‰
export WANDB_MODE=offline
python main.py

# ë‚˜ì¤‘ì— sync
wandb sync wandb/latest-run
```

### **2. Custom Run Name**

```python
# config.py
WANDB_RUN_NAME = "bert-large_gnn4_a6000_v1"
```

ë˜ëŠ”

```python
# main.pyì—ì„œ ë™ì  ìƒì„±
run_name = f"taxo_{Config.DOC_ENCODER_MODEL.split('/')[-1]}_gnn{Config.GNN_NUM_LAYERS}_lr{Config.LEARNING_RATE}"
```

### **3. Group & Tags**

```python
# config.py
WANDB_TAGS = ["bert-large", "a6000", "transductive", "gnn"]

# main.py
wandb.init(
    project=Config.WANDB_PROJECT,
    name=run_name,
    tags=Config.WANDB_TAGS,
    group="bert-large_experiments",  # ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸°
)
```

### **4. Resume Training**

```python
# main.py
wandb.init(
    project=Config.WANDB_PROJECT,
    id="unique_run_id",  # ì´ì „ runì˜ ID
    resume="must"  # ë°˜ë“œì‹œ resume
)
```

---

## ğŸ› **Troubleshooting**

### **ë¬¸ì œ 1: wandbê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ**

```bash
# ì¦ìƒ
ModuleNotFoundError: No module named 'wandb'

# í•´ê²°
pip install wandb
```

### **ë¬¸ì œ 2: ë¡œê·¸ì¸ ì•ˆë¨**

```bash
# ì¦ìƒ
wandb: ERROR Unable to authenticate

# í•´ê²°
wandb login
# ë˜ëŠ”
export WANDB_API_KEY=<your_api_key>
```

### **ë¬¸ì œ 3: ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨**

```bash
# ì¦ìƒ
wandb: WARNING Network error

# í•´ê²° (ì˜¤í”„ë¼ì¸ ëª¨ë“œ)
export WANDB_MODE=offline
python main.py
```

### **ë¬¸ì œ 4: ë¡œê·¸ê°€ ë„ˆë¬´ ë§ìŒ**

```python
# config.py
WANDB_LOG_INTERVAL = 100  # 10 â†’ 100 (ëœ ìì£¼ ë¡œê¹…)
WANDB_LOG_GRADIENTS = False  # Gradient ë¡œê¹… ë„ê¸°
```

### **ë¬¸ì œ 5: wandb ì™„ì „íˆ ë¹„í™œì„±í™”**

```python
# config.py
USE_WANDB = False

# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜
export WANDB_MODE=disabled
python main.py
```

---

## ğŸ“Š **ì˜ˆì œ: ì‹¤í—˜ ê²°ê³¼ ë¶„ì„**

### **Scenario: bert-base vs bert-large ë¹„êµ**

#### **Run 1: bert-base**
```python
# config.py
DOC_ENCODER_MODEL = "bert-base-uncased"
EMBEDDING_DIM = 768
GNN_HIDDEN_DIM = 512
BATCH_SIZE = 32

# ê²°ê³¼ (wandb)
test/accuracy: 0.752
stage3/best_val_loss: 0.348
Training time: 3.2 hours
```

#### **Run 2: bert-large (A6000 ìµœì í™”)**
```python
# config.py
DOC_ENCODER_MODEL = "bert-large-uncased"
EMBEDDING_DIM = 1024
GNN_HIDDEN_DIM = 1024
BATCH_SIZE = 64

# ê²°ê³¼ (wandb)
test/accuracy: 0.817  # +6.5%p í–¥ìƒ! â­
stage3/best_val_loss: 0.291  # ë” ë‚®ì€ loss
Training time: 2.1 hours  # Mixed precision ë•ë¶„ì— ë” ë¹ ë¦„!
```

#### **Wandb ë¹„êµ ì°¨íŠ¸**

```
Compare Runs:
- X-axis: training time
- Y-axis: test/accuracy
- Color: model (bert-base vs bert-large)

â†’ bert-largeê°€ ë” ë¹ ë¥´ê³  ì„±ëŠ¥ë„ ë†’ìŒ!
```

---

## ğŸ¯ **Best Practices**

### **1. ì²´ê³„ì ì¸ ì‹¤í—˜ ê´€ë¦¬**

```python
# ëª…í™•í•œ run name
run_name = f"{model_name}_gnn{n_layers}_lr{lr}_bs{batch_size}_v{version}"

# ìœ ì˜ë¯¸í•œ tags
tags = ["baseline", "bert-large", "a6000", "transductive"]

# ì‹¤í—˜ ê·¸ë£¹
group = "bert-large_ablation"  # ê°™ì€ ì‹¤í—˜êµ°
```

### **2. ì¤‘ìš” ë©”íŠ¸ë¦­ ìš°ì„ **

```python
# Summaryì— ìµœì¢… ê²°ê³¼ ê¸°ë¡
wandb.run.summary["final_accuracy"] = test_accuracy
wandb.run.summary["final_f1"] = test_f1
wandb.run.summary["training_time_hours"] = training_time
```

### **3. ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´**

```python
# Config ì €ì¥
wandb.config.update({
    "seed": Config.SEED,
    "git_commit": get_git_commit(),  # Git commit hash
    "cuda_version": torch.version.cuda,
    "pytorch_version": torch.__version__,
})

# Artifactsë¡œ ëª¨ë¸ ì €ì¥
artifact = wandb.Artifact("taxo_model", type="model")
artifact.add_file("saved_models/best_model.pt")
wandb.log_artifact(artifact)
```

---

## ğŸ“š **ì¶”ê°€ ìë£Œ**

- **Wandb ê³µì‹ ë¬¸ì„œ**: [https://docs.wandb.ai](https://docs.wandb.ai)
- **PyTorch Integration**: [https://docs.wandb.ai/guides/integrations/pytorch](https://docs.wandb.ai/guides/integrations/pytorch)
- **Sweeps Guide**: [https://docs.wandb.ai/guides/sweeps](https://docs.wandb.ai/guides/sweeps)

---

## âœ… **Quick Start Checklist**

- [ ] wandb ì„¤ì¹˜: `pip install wandb`
- [ ] ë¡œê·¸ì¸: `wandb login`
- [ ] config.pyì—ì„œ `USE_WANDB = True` ì„¤ì •
- [ ] í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •: `WANDB_PROJECT = "your-project"`
- [ ] í•™ìŠµ ì‹¤í–‰: `python main.py`
- [ ] Dashboard í™•ì¸: [https://wandb.ai](https://wandb.ai)
- [ ] ì‹¤í—˜ ë¹„êµ ë° ë¶„ì„

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-22  
**ë²„ì „**: 1.0  
**Wandbë¥¼ í†µí•´ íš¨ìœ¨ì ì¸ ì‹¤í—˜ ê´€ë¦¬ë¥¼ ê²½í—˜í•˜ì„¸ìš”!** ğŸš€

